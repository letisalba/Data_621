---
title: "Data 621 - Homework 4"
author: "Group 2: William Aiken, Donald Butler, Michael Ippolito, Bharani Nittala,
  and Leticia Salazar"
date: "November 6, 2022"
output:
  html_document:
    theme: yeti
    highlight: tango
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE, ref.label = knitr::all_labels(appendix == TRUE)}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## Overview:

In this homework assignment, you will explore, analyze and model a data set containing approximately 8,000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

## Objective: 

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

\newpage

## Description:

Below is a short description of the variables of interest in the data set:

| VARIABLE NAME:| DEFINITION:                             | THEORETICAL EFFECT:                                                                             |
|:---           |:---:                                    |:---:                                                                                            |
|INDEX          |Identification Variable (do not use)     |None                                                                                             |
|TARGET_FLAG    |Was Car in a crash? 1 = YES 2 = NO       |None                                                                                             |
|TARGET_AMT     |If car was in a crash, what was the cost |None                                                                                             |
|AGE            |Age of Driver                            |Very young people tend to be risky. Maybe very old people also.                                  |
|BLUEBOOK       |Value of Vehicle                         |Unknown effect on probability of collision, but probably effect the payout if there is a crash   |
|CAR_AGE        |Vehicle Age                              |Unknown effect on probability of collision, but probably effect the payout if there is a crash   |
|CAR_TYPE       |Type of Car                              |Unknown effect on probability of collision, but probably effect the payout if there is a crash   |
|CAR_USE        |Vehicle Use                              |Commercial vehicles are driven more, so might increase probability of collision                  |
|CLM_FREQ       |# Claims (Past 5 Years)                  |The more claims you filed in the past, the more you are likely to file in the future             |
|EDUCATION      |Max Education Level                      |Unknown effect, but in theory more educated people tend to drive more safely                     |
|HOMEKIDS       |# Children at Home                       |Unknown effect                                                                                   |
|HOME_VAL       |Home Value                               |In theory, home owners tend to drive more responsibly                                            |
|INCOME         |Income                                   |In theory, rich people tend to get into fewer crashes                                            |
|JOB            |Job Category                             |In theory, white collar jobs tend to be safer                                                    |
|KIDSDRIV       |# Driving Children                       |When teenagers drive your car, you are more likely to get into crashes                           |
|MSTATUS        |Martial Status                           |In theory, married people drive more safely                                                      |
|MVR_PTS        |Motor Vehicle Record Points              |If you get lots of traffic tickets, you tend to get into more crashes                            |
|OLDCLAIM       |Total Claims (Past 5 Years)              |If your total payout over the past five years was high, this suggests future payouts will be high|
|PARENT1        |Single Parent                            |Unknown effect                                                                                   |
|RED_CAR        |A Red Car                                |Urban legend says that red cars (especially red sports cars) are more risky. Is that true?       |
|REVOKED        |License Revoked (Past 7 Years)           |If your license was revoked in the past 7 years, you probably are a more risky driver.           |
|SEX            |Gender                                   |Urban legend says that women have less crashes then men. Is that true?                           |
|TIF            |Time in Force                            |People who have been customers for a long time are usually more safe.                            |
|TRAVTIME       |Distance to Work                         |Long drives to work usually suggest greater risk                                                 |
|URBANICITY     |Home / Work Area                         |Unknown                                                                                          |
|YOJ            |Years on Job                             |People who stay at a job for a long time are usually more safe                                   |

-----------

### Load Libraries:

These are the libraries used to explore, prepare, analyze and build our models
```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(pROC)
library(corrplot)
library(GGally)
library(psych)
library(car)
library(kableExtra)
library(gridExtra)
library(performance)
library(faraway)
library(jtools)
library(DataExplorer)
library(hrbrthemes)
library(MASS)
```

### Load Data set:

We have included the original data sets in our GitHub account and read from this location. Our training data set includes 8,161 records and 26 variables.
```{r, loading data, echo=FALSE}
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_4/csv/insurance_training_data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_4/csv/insurance-evaluation-data.csv")
glimpse(dftrain)
```

-----------


## Data Exploration:

For insight on the data we use the `summary()` function on the train dataset:
```{r, echo=FALSE}
summary(dftrain)
```


Let's look at the statistical summary for the `dfeval` data as well:
```{r, echo=FALSE}
summary(dfeval)
```

$~$

Distributions for training data set:

```{r fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center'}
DataExplorer::plot_bar(
  data = dftrain,
         order_bar = T,
         ggtheme=theme_ipsum())
```

```{r, fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center'}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 0.5),
   data = dftrain,
         ggtheme=theme_ipsum())
```

Distributions for evaluation data set:

```{r fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center'}
DataExplorer::plot_bar(
  data = dfeval,
         order_bar = T,
         ggtheme=theme_ipsum())
```


```{r, fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center'}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 0.5),
   data = dfeval,
         ggtheme=theme_ipsum())
```



$~$

The following dummy variables are done to both the training and evaluation data set:

#### PARENT1

The *PARENT1* variable has two values, Yes and No, to indicate if the observation is a single parent. We will construct a dummy variable *SingleParent* = 1 if *PARENT1* = Yes.

```{r, echo=FALSE}
dftrain %>% count(PARENT1)
```

```{r, echo=FALSE}
dfeval %>% count(PARENT1)
```

$~$

#### SEX

The *SEX* variable has two values, M and z_F. We will create a dummy variable *Male* = 1 if *SEX* = M.

```{r, echo=FALSE}
dftrain %>% count(SEX)
```
```{r, echo=FALSE}
dfeval %>% count(SEX)
```

$~$

#### MSTATUS

The variable *MSTATUS* has two values, Yes and z_No, to indicate the marital status. We will create a dummy variable *Married* = 1 if MSTATUS = Yes.

```{r, echo=FALSE}
dftrain %>% count(MSTATUS)
```
```{r, echo=FALSE}
dfeval %>% count(MSTATUS)
```

$~$

#### EDUCATION

The *EDUCATION* variable takes on 5 values ranging from less than high school through PHD. We will construct dummy variables: *HighSchool*, *Bachelors*, *Masters*, *PHD*, to indicate the highest level of education completed.

```{r, echo=FALSE}
dftrain %>% count(EDUCATION)
```
```{r, echo=FALSE}
dfeval %>% count(EDUCATION)
```

$~$

#### JOB

The *JOB* variable takes on 8 values. The *JOB* variable has 526 missing values, so we will construct dummy variables for all 8 values assuming the missing values are not one of the listed professions. The dummy variables we will create are: *Clerical*, *Doctor*, *HomeMaker*, *Lawyer*, *Manager*, *Professional*, *Student*, and *BlueCollar*.

```{r, echo=FALSE}
dftrain %>% count(JOB)
```
```{r, echo=FALSE}
dfeval %>% count(JOB)
```
$~$

#### CAR_USE

The *CAR_USE* variable has two values, Commercial and Private. We will construct a dummy variable *Commercial* = 1 if Commercial.

```{r, echo=FALSE}
dftrain %>% count(CAR_USE)
```
```{r, echo=FALSE}
dfeval %>% count(CAR_USE)
```
$~$

#### CAR_TYPE

The *CAR_TYPE* variable takes on 6 values. We will create dummy variables; *Minivan*, *PanelTruck*, *Pickup*, *SportsCar*, and *Van*.

```{r, echo=FALSE}
dftrain %>% count(CAR_TYPE)
```

```{r, echo=FALSE}
dfeval %>% count(CAR_TYPE)
```

$~$

#### RED_CAR

The *RED_CAR* variable has two values, yes and no. We will create a dummy variable *RedCar* = 1 if *RED_CAR* = yes.

```{r, echo=FALSE}
dftrain %>% count(RED_CAR)
```

```{r, echo=FALSE}
dfeval %>% count(RED_CAR)
```

$~$

#### REVOKED

The *REVOKED* variable has two values, Yes and No. We will create a dummy variable *DLRevoked* = 1 if *REVOKED* = Yes.

```{r, echo=FALSE}
dftrain %>% count(REVOKED)
```
```{r, echo=FALSE}
dfeval %>% count(REVOKED)
```

$~$

#### URBANICITY

The *URBANICITY* variable has two values, Highly Urban/ Urban and z_Highly Rural/ Rural. We will create a dummy variable *Urban* = 1 if *URBANICITY* = Highly Urban/ Urban.

```{r, echo=FALSE}
dftrain %>% count(URBANICITY)
```
```{r, echo=FALSE}
dfeval %>% count(URBANICITY)
```

-----------

## Data Preparation:

Performed to both the training and evaluation data sets but will only be displayed for the training data.

$~$

#### Data Cleaning Function

Checking for missing values for the training data set. This includes NAs that might be introduced as a result of conversion to numeric.

```{r, echo=FALSE}
# Display NAs
colSums(is.na(dftrain))

# Look for blank values in columns that will be converted to numeric, which would introduce NAs
dollar_cols <- c('BLUEBOOK', 'HOME_VAL', 'INCOME', 'OLDCLAIM')
ctVals <- c()
for (dollar_col in dollar_cols) {
    ctVals <- c(ctVals, sum(str_length(dftrain[,dollar_col]) == 0))
}
ctVals <- data.frame(t(ctVals))
colnames(ctVals) <- dollar_cols
ctVals
```

* The attributes `BLUEBOOK`, `HOME_VAL`, `INCOME`, and` OLDCLAIM` are dollar amounts stored as characters. Need to convert to int. It is noted that converting blank values to numeric introduces some NAs; therefore our cleaning function will handle these cases.
* Variables with NA: `AGE` (6), `YOJ` (454), `CAR_AGE` (510)
* Consider creating `AGE` groups Under25 and Over65 to account for young and older drivers.
* Consider creating `CAR_AGE` groups to identify new cars. One observation has a `CAR_AGE` = -3, which shouldn't be possible.
* Consider creating `YOJ` (Year on Job) groups to identify job stability; Over5years etc.


```{r, echo=FALSE}
clean_df <- function(df) {

  # Note that some NAs were introduced after cleaning the dollar and commas from these variables
  df$BLUEBOOK <- as.numeric(gsub('[$,]','',df$BLUEBOOK))
  df$HOME_VAL <- as.numeric(gsub('[$,]','',df$HOME_VAL))
  df$INCOME <- as.numeric(gsub('[$,]','',df$INCOME))
  df$OLDCLAIM <- as.numeric(gsub('[$,]','',df$OLDCLAIM))

  df <- df %>%
    mutate(SingleParent = ifelse(PARENT1 == 'Yes', 1, 0)) %>% dplyr::select(-PARENT1) %>%
    mutate(Male = ifelse(SEX == 'M', 1, 0)) %>% dplyr::select(-SEX) %>%
    mutate(Married = ifelse(MSTATUS == 'Yes', 1, 0)) %>% dplyr::select(-MSTATUS) %>%
    mutate(HighSchool = ifelse(EDUCATION == 'z_High School', 1, 0)) %>%
    mutate(Bachelors = ifelse(EDUCATION == 'Bachelors', 1, 0)) %>%
    mutate(Masters = ifelse(EDUCATION == 'Masters', 1, 0)) %>%
    mutate(PHD = ifelse(EDUCATION == 'PhD', 1, 0)) %>% dplyr::select(-EDUCATION) %>%
    mutate(Clerical = ifelse(JOB == 'Clerical', 1, 0)) %>%
    mutate(Doctor = ifelse(JOB == 'Doctor', 1, 0)) %>%
    mutate(HomeMaker = ifelse(JOB == 'Home Maker', 1, 0)) %>%
    mutate(Lawyer = ifelse(JOB == 'Lawyer', 1, 0)) %>%
    mutate(Manager = ifelse(JOB == 'Manager', 1, 0)) %>%
    mutate(Professional = ifelse(JOB == 'Professional', 1, 0)) %>%
    mutate(Student = ifelse(JOB == 'Student', 1, 0)) %>%
    mutate(BlueCollar = ifelse(JOB == 'z_Blue Collar', 1, 0)) %>% dplyr::select(-JOB) %>%
    mutate(Commercial = ifelse(CAR_USE == 'Commercial', 1, 0)) %>% dplyr::select(-CAR_USE) %>%
    mutate(Minivan = ifelse(CAR_TYPE == 'Minivan', 1, 0)) %>%
    mutate(PanelTruck = ifelse(CAR_TYPE == 'Panel Truck', 1, 0)) %>%
    mutate(Pickup = ifelse(CAR_TYPE == 'Pickup', 1, 0)) %>%
    mutate(SportsCar = ifelse(CAR_TYPE == 'Sports Car', 1, 0)) %>%
    mutate(Van = ifelse(CAR_TYPE == 'Van', 1, 0)) %>% dplyr::select(-CAR_TYPE) %>%
    mutate(RedCar = ifelse(RED_CAR == 'yes', 1, 0)) %>% dplyr::select(-RED_CAR) %>%
    mutate(DLRevoked = ifelse(REVOKED == 'Yes', 1, 0)) %>% dplyr::select(-REVOKED) %>%
    mutate(Urban = ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0)) %>% dplyr::select(-URBANICITY)
  
  # Change negative values of car age to NA
  df$CAR_AGE <- ifelse(df$CAR_AGE < 0, NA, df$CAR_AGE)
  
  # Handle NAs
  df$AGE <- ifelse(is.na(df$AGE), median(df$AGE, na.rm=T), df$AGE)
  df$YOJ <- ifelse(is.na(df$YOJ), median(df$YOJ, na.rm=T), df$YOJ)
  df$INCOME <- ifelse(is.na(df$INCOME), median(df$INCOME, na.rm=T), df$INCOME)
  df$HOME_VAL <- ifelse(is.na(df$HOME_VAL), median(df$HOME_VAL, na.rm=T), df$HOME_VAL)
  df$CAR_AGE <- ifelse(is.na(df$CAR_AGE), median(df$CAR_AGE, na.rm=T), df$CAR_AGE)
  
  return(df)
}

cleandf <- clean_df(dftrain)
clean_evaldf <- clean_df(dfeval)
head(cleandf)
#summary(cleandf)

```


Checking for missing values for the evaluation data set, including NAs that might be introduced as a result of conversion to numeric. The TARGET_FLAG and TARGET_AMT are the response variables so should remain NA for now.

```{r, echo=FALSE}
# Show NAs
colSums(is.na(dfeval))

# Look for blank values in columns that will be converted to numeric, which would introduce NAs
dollar_cols <- c('BLUEBOOK', 'HOME_VAL', 'INCOME', 'OLDCLAIM')
ctVals <- c()
for (dollar_col in dollar_cols) {
    ctVals <- c(ctVals, sum(str_length(dfeval[,dollar_col]) == 0))
}
ctVals <- data.frame(t(ctVals))
colnames(ctVals) <- dollar_cols
ctVals

```


* The attributes `BLUEBOOK`, `HOME_VAL`, `INCOME`, and` OLDCLAIM` are dollar amounts stored as characters. Need to convert to int. It is noted that converting blank values to numeric introduces some NAs; therefore our cleaning function will handle these cases.
* Variables with NA: `AGE` (1), `YOJ` (94), `CAR_AGE` (125)

```{r, echo=FALSE, eval=FALSE, warning=FALSE}
eval_clean_df <- function(df) {

  df$BLUEBOOK <- as.numeric(gsub('[$,]','',df$BLUEBOOK))
  df$HOME_VAL <- as.numeric(gsub('[$,]','',df$HOME_VAL))
  df$INCOME <- as.numeric(gsub('[$,]','',df$INCOME))
  df$OLDCLAIM <- as.numeric(gsub('[$,]','',df$OLDCLAIM))

  df <- df %>%
    mutate(SingleParent = ifelse(PARENT1 == 'Yes', 1, 0)) %>% dplyr::select(-PARENT1) %>%
    mutate(Male = ifelse(SEX == 'M', 1, 0)) %>% dplyr::select(-SEX) %>%
    mutate(Married = ifelse(MSTATUS == 'Yes', 1, 0)) %>% dplyr::select(-MSTATUS) %>%
    mutate(HighSchool = ifelse(EDUCATION == 'z_High School', 1, 0)) %>%
    mutate(Bachelors = ifelse(EDUCATION == 'Bachelors', 1, 0)) %>%
    mutate(Masters = ifelse(EDUCATION == 'Masters', 1, 0)) %>%
    mutate(PHD = ifelse(EDUCATION == 'PhD', 1, 0)) %>% dplyr::select(-EDUCATION) %>%
    mutate(Clerical = ifelse(JOB == 'Clerical', 1, 0)) %>%
    mutate(Doctor = ifelse(JOB == 'Doctor', 1, 0)) %>%
    mutate(HomeMaker = ifelse(JOB == 'Home Maker', 1, 0)) %>%
    mutate(Lawyer = ifelse(JOB == 'Lawyer', 1, 0)) %>%
    mutate(Manager = ifelse(JOB == 'Manager', 1, 0)) %>%
    mutate(Professional = ifelse(JOB == 'Professional', 1, 0)) %>%
    mutate(Student = ifelse(JOB == 'Student', 1, 0)) %>%
    mutate(BlueCollar = ifelse(JOB == 'z_Blue Collar', 1, 0)) %>% dplyr::select(-JOB) %>%
    mutate(Commercial = ifelse(CAR_USE == 'Commercial', 1, 0)) %>% dplyr::select(-CAR_USE) %>%
    mutate(Minivan = ifelse(CAR_TYPE == 'Minivan', 1, 0)) %>%
    mutate(PanelTruck = ifelse(CAR_TYPE == 'Panel Truck', 1, 0)) %>%
    mutate(Pickup = ifelse(CAR_TYPE == 'Pickup', 1, 0)) %>%
    mutate(SportsCar = ifelse(CAR_TYPE == 'Sports Car', 1, 0)) %>%
    mutate(Van = ifelse(CAR_TYPE == 'Van', 1, 0)) %>% dplyr::select(-CAR_TYPE) %>%
    mutate(RedCar = ifelse(RED_CAR == 'yes', 1, 0)) %>% dplyr::select(-RED_CAR) %>%
    mutate(DLRevoked = ifelse(REVOKED == 'Yes', 1, 0)) %>% dplyr::select(-REVOKED) %>%
    mutate(Urban = ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0)) %>% dplyr::select(-URBANICITY)
  
  # Handle NAs
  df$AGE <- ifelse(is.na(df$AGE), median(df$AGE, na.rm=T), df$AGE)
  df$YOJ <- ifelse(is.na(df$YOJ), median(df$YOJ, na.rm=T), df$YOJ)
  df$INCOME <- ifelse(is.na(df$INCOME), median(df$INCOME, na.rm=T), df$INCOME)
  df$HOME_VAL <- ifelse(is.na(df$HOME_VAL), median(df$HOME_VAL, na.rm=T), df$HOME_VAL)
  df$CAR_AGE <- ifelse(is.na(df$CAR_AGE), median(df$CAR_AGE, na.rm=T), df$CAR_AGE)
  
  return(df)
}

eval_cleandf <- eval_clean_df(dfeval)
head(eval_cleandf)
#summary(eval_cleandf)

```


$~$

The correlation plot below is measuring the degree of linear relationship within the cleaned training data. The values in which this is measured falls between -1 and +1, with +1 being a strong positive correlation and -1 a strong negative correlation.


```{r corr-plot, fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center'}
cor_res <- cor(cleandf, use = "na.or.complete")

corrplot(cor_res,
         type = "lower",
         order = "original",
         tl.col = "black",
         tl.srt = 50,
         tl.cex = 1)
cor_res <- data.frame(cor_res)
```
$~$

-----------

## Model Building:

We will be building six different models; three multiple linear regression (MLR) models and three binary logistic regression (BLR) models.

### **Model 1**

MLR: Base model where all variables are tested
```{r, echo=FALSE}
model1 <- lm(TARGET_AMT ~., cleandf, na.action = na.omit)
summary(model1)
#summ(model1)
```

$~$

### **Model 2**

MLR: Backward Elimination
```{r, echo=FALSE}
model2 <- lm(TARGET_AMT ~ KIDSDRIV + HOMEKIDS + INCOME + SingleParent + HOME_VAL + Married + TRAVTIME + BLUEBOOK + TIF + CLM_FREQ + MVR_PTS + CAR_AGE, cleandf, na.action = na.omit)
summary(model2)
#summ(model2)
```

$~$

### **Model 3**

MLR: BoxCox Transformation
```{r, echo=FALSE}
model_boxcox <- preProcess(cleandf, c("BoxCox"))
model_bc_transformed <- predict(model_boxcox, cleandf)
model3 <- lm(TARGET_AMT ~ ., model_bc_transformed)
summary(model3)
#summ(model3)
```

$~$

### **Model 4**
BLR: Base model where all variables are tested
```{r, echo=FALSE}
reduced_cleandf <- cleandf %>% dplyr::select(-c("INDEX", "TARGET_AMT"))
model4 <- glm(TARGET_FLAG ~., reduced_cleandf, family = binomial(link = "logit"))
summary(model4)
#summ(model4)
```

$~$

### **Model 5**

BLR: drop term model

The `dropterm()` function from the MASS package tests all models from the current model selected by fitting all models that differ from the current model by dropping a single term, maintaining marginality.

```{r, echo=FALSE, warning=FALSE, eval=FALSE}
dropterm(model4, test = "F")
```


```{r, echo=FALSE}
model5 <- glm(TARGET_FLAG ~  KIDSDRIV + AGE + HOMEKIDS + 
    YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + 
    CLM_FREQ + MVR_PTS + CAR_AGE + SingleParent + Male + Married + 
    HighSchool + Bachelors + Masters + PHD + Clerical + Doctor + 
    HomeMaker + Lawyer + Manager + Professional + Student + BlueCollar + 
    Commercial + Minivan + PanelTruck + Pickup + SportsCar + 
    Van + RedCar + DLRevoked + Urban, cleandf, family = binomial(link = "logit"))
summary(model5)
#summ(model5)
```

$~$

### **Model 6**
BLR: Backward Elimination
```{r, echo=FALSE, warning=FALSE}
model6 <- glm(TARGET_FLAG ~  KIDSDRIV + AGE + HOMEKIDS + 
    YOJ + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + 
    CLM_FREQ + CAR_AGE + SingleParent + Male + Married + Bachelors + Clerical + Doctor + 
    HomeMaker + Lawyer + Manager + Student + BlueCollar + 
    Commercial + Minivan + PanelTruck + Pickup + SportsCar + 
    Van + RedCar + Urban, data = cleandf, family=binomial(link="logit"))
summary(model6)
#summ(model6)
```
Note: Models 3 and 6 should be the best fit

$~$
-----------

## Select Models: 

* Lets examine our linear models

```{r, echo=FALSE}
performance::check_model(model1)
```
```{r, echo=FALSE}
performance::check_model(model2)
```

```{r, echo=FALSE}
performance::check_model(model3)
```

Let's evaluate the performance of our linear models

```{r, echo=FALSE}
performance::model_performance(model1)
```

```{r, echo=FALSE}
performance::model_performance(model2)
```


```{r, echo=FALSE}
performance::model_performance(model3)
```

 Lets examine our binary logistic models

```{r, echo=FALSE}
performance::check_model(model4)
```

```{r, echo=FALSE}
performance::check_model(model5)
```

```{r, echo=FALSE}
performance::check_model(model6)
```

Let's evaluate the performance of our binary logistic models

```{r, echo=FALSE}
performance::model_performance(model4)
```

```{r, echo=FALSE}
pred = round(fitted(model4))
pROC::auc(cleandf$TARGET_FLAG, pred)
```

```{r, echo=FALSE}
table(true = cleandf$TARGET_FLAG, pred = round(fitted(model4)))
```

Accuracy: $\frac{5551+918}{8161}=79\%$

Classification Error Rate: $\frac{1235 + 457}{8161}=20\%$

Precision: $\frac{918}{918 + 457}=67%$

Sensitivity: $\frac{918}{918 + 1235}=43\%$

Specificity: $\frac{5551}{5551 + 457}=92\%$

F1 Score: $\frac{2 * 918}{2 * 918 + 457 + 1235}=52\%$

```{r, echo=FALSE}
performance::model_performance(model5)
```

```{r, echo=FALSE}
pred = round(fitted(model5))
pROC::auc(cleandf$TARGET_FLAG, pred)
```

```{r, echo=FALSE}
table(true = cleandf$TARGET_FLAG, pred = round(fitted(model5)))
```

Accuracy: $\frac{5551+918}{8161}=79\%$

Classification Error Rate: $\frac{1235 + 457}{8161}=20\%$

Precision: $\frac{918}{918 + 457}=67%$

Sensitivity: $\frac{918}{918 + 1235}=43\%$

Specificity: $\frac{5551}{5551 + 457}=92\%$

F1 Score: $\frac{2 * 918}{2 * 918 + 457 + 1235}=52\%$


```{r, echo=FALSE}
performance::model_performance(model6)
```

```{r, echo=FALSE}
pred = round(fitted(model6))
pROC::auc(cleandf$TARGET_FLAG, pred)
```


```{r,echo=FALSE}
table(true = cleandf$TARGET_FLAG, pred = round(fitted(model6)))
```

Accuracy: $\frac{5535+848}{8161}=78\%$

Classification Error Rate: $\frac{1305 + 473}{8161}=22\%$

Precision: $\frac{848}{848 + 473}=64%$

Sensitivity: $\frac{848}{848 + 1305}=39\%$

Specificity: $\frac{5535}{5535 + 473}=92\%$

F1 Score: $\frac{2 * 848}{2 * 848 + 473 + 1305}=49\%$

Lets make predictions with our two chosen models:

```{r, echo=FALSE}
clean_evaldf$TARGET_FLAG <- as.numeric(clean_evaldf$TARGET_FLAG)
reduced_clean_evaldf <- clean_evaldf %>% dplyr::select(-c( "TARGET_AMT"))

prediction <- broom::augment(model6, newdata = reduced_clean_evaldf)
head(as.integer(prediction$.fitted > .5), 20)

TARGET_FLAG <- as.data.frame(as.integer(prediction$.fitted > .5))
names(TARGET_FLAG) <- "TARGET_FLAG"
```

```{r, echo=FALSE}
clean_evaldf$TARGET_AMT <- as.numeric(clean_evaldf$TARGET_AMT)
clean_evaldf$TARGET_FLAG <- NULL

clean_evaldf <- bind_cols(TARGET_FLAG, clean_evaldf)
prediction <- broom::augment(model3, newdata = clean_evaldf)
head(prediction$.fitted, 20)
```


-----------

$~$

## Appendix:

Below is all the code used in this homework
```{r, eval=FALSE, warning=FALSE}
# load libraries
library(tidyverse)
library(caret)
library(pROC)
library(corrplot)
library(GGally)
library(psych)
library(car)
library(kableExtra)
library(gridExtra)
library(performance)
library(faraway)
library(jtools)
library(DataExplorer)
library(hrbrthemes)
library(MASS)

# load data
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_4/csv/insurance_training_data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_4/csv/insurance_training_data.csv")
glimpse(dftrain)

# summary of training data
summary(dftrain)

# bar plots for training set
DataExplorer::plot_bar(
  data = dftrain,
         order_bar = T,
         ggtheme=theme_ipsum())

# histograms for training set
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 0.5),
   data = dftrain,
         ggtheme=theme_ipsum())

# summary of evaluation data
summary(dfeval)

# bar plots for evaluation set
DataExplorer::plot_bar(
  data = dftrain,
         order_bar = T,
         ggtheme=theme_ipsum())

# histograms for evaluation set
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 0.5),
   data = dftrain,
         ggtheme=theme_ipsum())

# creating dummy variables for some variables in training and evaluation data sets
dftrain %>% count(PARENT1)
dfeval %>% count(PARENT1)

dftrain %>% count(SEX)
dfeval %>% count(SEX)
  
dftrain %>% count(MSTATUS)
dfeval %>% count(MSTATUS)
  
dftrain %>% count(EDUCATION)
dfeval %>% count(EDUCATION)
  
dftrain %>% count(JOB)
dfeval %>% count(JOB)
  
dftrain %>% count(CAR_USE)
dfeval %>% count(CAR_USE)
  
dftrain %>% count(CAR_TYPE)
dfeval %>% count(CAR_TYPE)
  
dftrain %>% count(RED_CAR)
dfeval %>% count(RED_CAR)
  
dftrain %>% count(REVOKED)
dfeval %>% count(REVOKED)
  
dftrain %>% count(URBANICITY)
dfeval %>% count(URBANICITY)
  
# checking for missing values in training data
colSums(is.na(dftrain))

# Look for blank values in columns that will be converted to numeric, which would introduce NAs
dollar_cols <- c('BLUEBOOK', 'HOME_VAL', 'INCOME', 'OLDCLAIM')
ctVals <- c()
for (dollar_col in dollar_cols) {
    ctVals <- c(ctVals, sum(str_length(dftrain[,dollar_col]) == 0))
}
ctVals <- data.frame(t(ctVals))
colnames(ctVals) <- dollar_cols
ctVals

# data cleaning functions for training data
clean_df <- function(df) {

  df$BLUEBOOK <- as.numeric(gsub('[$,]','',df$BLUEBOOK))
  df$HOME_VAL <- as.numeric(gsub('[$,]','',df$HOME_VAL))
  df$INCOME <- as.numeric(gsub('[$,]','',df$INCOME))
  df$OLDCLAIM <- as.numeric(gsub('[$,]','',df$OLDCLAIM))

  df <- df %>%
    mutate(SingleParent = ifelse(PARENT1 == 'Yes', 1, 0)) %>% dplyr::select(-PARENT1) %>%
    mutate(Male = ifelse(SEX == 'M', 1, 0)) %>% dplyr::select(-SEX) %>%
    mutate(Married = ifelse(MSTATUS == 'Yes', 1, 0)) %>% dplyr::select(-MSTATUS) %>%
    mutate(HighSchool = ifelse(EDUCATION == 'z_High School', 1, 0)) %>%
    mutate(Bachelors = ifelse(EDUCATION == 'Bachelors', 1, 0)) %>%
    mutate(Masters = ifelse(EDUCATION == 'Masters', 1, 0)) %>%
    mutate(PHD = ifelse(EDUCATION == 'PhD', 1, 0)) %>% dplyr::select(-EDUCATION) %>%
    mutate(Clerical = ifelse(JOB == 'Clerical', 1, 0)) %>%
    mutate(Doctor = ifelse(JOB == 'Doctor', 1, 0)) %>%
    mutate(HomeMaker = ifelse(JOB == 'Home Maker', 1, 0)) %>%
    mutate(Lawyer = ifelse(JOB == 'Lawyer', 1, 0)) %>%
    mutate(Manager = ifelse(JOB == 'Manager', 1, 0)) %>%
    mutate(Professional = ifelse(JOB == 'Professional', 1, 0)) %>%
    mutate(Student = ifelse(JOB == 'Student', 1, 0)) %>%
    mutate(BlueCollar = ifelse(JOB == 'z_Blue Collar', 1, 0)) %>% dplyr::select(-JOB) %>%
    mutate(Commercial = ifelse(CAR_USE == 'Commercial', 1, 0)) %>% dplyr::select(-CAR_USE) %>%
    mutate(Minivan = ifelse(CAR_TYPE == 'Minivan', 1, 0)) %>%
    mutate(PanelTruck = ifelse(CAR_TYPE == 'Panel Truck', 1, 0)) %>%
    mutate(Pickup = ifelse(CAR_TYPE == 'Pickup', 1, 0)) %>%
    mutate(SportsCar = ifelse(CAR_TYPE == 'Sports Car', 1, 0)) %>%
    mutate(Van = ifelse(CAR_TYPE == 'Van', 1, 0)) %>% dplyr::select(-CAR_TYPE) %>%
    mutate(RedCar = ifelse(RED_CAR == 'yes', 1, 0)) %>% dplyr::select(-RED_CAR) %>%
    mutate(DLRevoked = ifelse(REVOKED == 'Yes', 1, 0)) %>% dplyr::select(-REVOKED) %>%
    mutate(Urban = ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0)) %>% dplyr::select(-URBANICITY)

  # Change negative values of car age to NA
  df$CAR_AGE <- ifelse(df$CAR_AGE < 0, NA, df$CAR_AGE)
  
  # Handle NAs
  df$AGE <- ifelse(is.na(df$AGE), median(df$AGE, na.rm=T), df$AGE)
  df$YOJ <- ifelse(is.na(df$YOJ), median(df$YOJ, na.rm=T), df$YOJ)
  df$INCOME <- ifelse(is.na(df$INCOME), median(df$INCOME, na.rm=T), df$INCOME)
  df$HOME_VAL <- ifelse(is.na(df$HOME_VAL), median(df$HOME_VAL, na.rm=T), df$HOME_VAL)
  df$CAR_AGE <- ifelse(is.na(df$CAR_AGE), median(df$CAR_AGE, na.rm=T), df$CAR_AGE)
  
  return(df)
}

cleandf <- clean_df(dftrain)
head(cleandf)

# checking for missing values in evaluation data
colSums(is.na(dfeval))

# Look for blank values in columns that will be converted to numeric, which would introduce NAs
dollar_cols <- c('BLUEBOOK', 'HOME_VAL', 'INCOME', 'OLDCLAIM')
ctVals <- c()
for (dollar_col in dollar_cols) {
    ctVals <- c(ctVals, sum(str_length(dfeval[,dollar_col]) == 0))
}
ctVals <- data.frame(t(ctVals))
colnames(ctVals) <- dollar_cols
ctVals

# data cleaning functions for evaluation data
eval_clean_df <- function(df) {

  df$BLUEBOOK <- as.numeric(gsub('[$,]','',df$BLUEBOOK))
  df$HOME_VAL <- as.numeric(gsub('[$,]','',df$HOME_VAL))
  df$INCOME <- as.numeric(gsub('[$,]','',df$INCOME))
  df$OLDCLAIM <- as.numeric(gsub('[$,]','',df$OLDCLAIM))

  df <- df %>%
    mutate(SingleParent = ifelse(PARENT1 == 'Yes', 1, 0)) %>% dplyr::select(-PARENT1) %>%
    mutate(Male = ifelse(SEX == 'M', 1, 0)) %>% dplyr::select(-SEX) %>%
    mutate(Married = ifelse(MSTATUS == 'Yes', 1, 0)) %>% dplyr::select(-MSTATUS) %>%
    mutate(HighSchool = ifelse(EDUCATION == 'z_High School', 1, 0)) %>%
    mutate(Bachelors = ifelse(EDUCATION == 'Bachelors', 1, 0)) %>%
    mutate(Masters = ifelse(EDUCATION == 'Masters', 1, 0)) %>%
    mutate(PHD = ifelse(EDUCATION == 'PhD', 1, 0)) %>% dplyr::select(-EDUCATION) %>%
    mutate(Clerical = ifelse(JOB == 'Clerical', 1, 0)) %>%
    mutate(Doctor = ifelse(JOB == 'Doctor', 1, 0)) %>%
    mutate(HomeMaker = ifelse(JOB == 'Home Maker', 1, 0)) %>%
    mutate(Lawyer = ifelse(JOB == 'Lawyer', 1, 0)) %>%
    mutate(Manager = ifelse(JOB == 'Manager', 1, 0)) %>%
    mutate(Professional = ifelse(JOB == 'Professional', 1, 0)) %>%
    mutate(Student = ifelse(JOB == 'Student', 1, 0)) %>%
    mutate(BlueCollar = ifelse(JOB == 'z_Blue Collar', 1, 0)) %>% dplyr::select(-JOB) %>%
    mutate(Commercial = ifelse(CAR_USE == 'Commercial', 1, 0)) %>% dplyr::select(-CAR_USE) %>%
    mutate(Minivan = ifelse(CAR_TYPE == 'Minivan', 1, 0)) %>%
    mutate(PanelTruck = ifelse(CAR_TYPE == 'Panel Truck', 1, 0)) %>%
    mutate(Pickup = ifelse(CAR_TYPE == 'Pickup', 1, 0)) %>%
    mutate(SportsCar = ifelse(CAR_TYPE == 'Sports Car', 1, 0)) %>%
    mutate(Van = ifelse(CAR_TYPE == 'Van', 1, 0)) %>% dplyr::select(-CAR_TYPE) %>%
    mutate(RedCar = ifelse(RED_CAR == 'yes', 1, 0)) %>% dplyr::select(-RED_CAR) %>%
    mutate(DLRevoked = ifelse(REVOKED == 'Yes', 1, 0)) %>% dplyr::select(-REVOKED) %>%
    mutate(Urban = ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0)) %>% dplyr::select(-URBANICITY)
  
  # Handle NAs
  df$AGE <- ifelse(is.na(df$AGE), median(df$AGE, na.rm=T), df$AGE)
  df$YOJ <- ifelse(is.na(df$YOJ), median(df$YOJ, na.rm=T), df$YOJ)
  df$INCOME <- ifelse(is.na(df$INCOME), median(df$INCOME, na.rm=T), df$INCOME)
  df$HOME_VAL <- ifelse(is.na(df$HOME_VAL), median(df$HOME_VAL, na.rm=T), df$HOME_VAL)
  df$CAR_AGE <- ifelse(is.na(df$CAR_AGE), median(df$CAR_AGE, na.rm=T), df$CAR_AGE)
  
  return(df)
}

eval_cleandf <- eval_clean_df(dfeval)
head(eval_cleandf)

# correlation plot for training data
cor_res <- cor(cleandf, use = "na.or.complete")

corrplot(cor_res,
         type = "lower",
         order = "original",
         tl.col = "black",
         tl.srt = 50,
         tl.cex = 1)
cor_res <- data.frame(cor_res)

# model building for multiple linear regression
model1 <- lm(TARGET_AMT ~., cleandf, na.action = na.omit)
summary(model1)
#summ(model1)

model2 <- lm(TARGET_AMT ~ KIDSDRIV + HOMEKIDS + INCOME + SingleParent + HOME_VAL + Married + TRAVTIME + BLUEBOOK + TIF + CLM_FREQ + MVR_PTS + CAR_AGE, cleandf, na.action = na.omit)
summary(model2)
#summ(model2)

model_boxcox <- preProcess(cleandf, c("BoxCox"))
model_bc_transformed <- predict(model_boxcox, cleandf)
model3 <- lm(TARGET_AMT ~ ., model_bc_transformed)
summary(model3)
#summ(model3)

# model building for binary logistic regression
model4 <- glm(TARGET_FLAG ~., cleandf, family = binomial(link = "logit"))
summary(model4)

dropterm(model4, test = "F")
model5 <- glm(TARGET_FLAG ~ INDEX + TARGET_AMT + KIDSDRIV + AGE + HOMEKIDS + 
    YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + 
    CLM_FREQ + MVR_PTS + CAR_AGE + SingleParent + Male + Married + 
    HighSchool + Bachelors + Masters + PHD + Clerical + Doctor + 
    HomeMaker + Lawyer + Manager + Professional + Student + BlueCollar + 
    Commercial + Minivan + PanelTruck + Pickup + SportsCar + 
    Van + RedCar + DLRevoked + Urban, cleandf, family = binomial(link = "logit"))
summary(model5)

model6 <- glm(TARGET_FLAG ~ INDEX + TARGET_AMT + KIDSDRIV + AGE + HOMEKIDS + 
    YOJ + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + 
    CLM_FREQ + CAR_AGE + SingleParent + Male + Married + Bachelors + Clerical + Doctor + 
    HomeMaker + Lawyer + Manager + Student + BlueCollar + 
    Commercial + Minivan + PanelTruck + Pickup + SportsCar + 
    Van + RedCar + Urban, data = cleandf, family=binomial(link="logit"))
summary(model6)

# model selection
performance::check_model(model1)

performance::check_model(model2)

performance::check_model(model3)


performance::model_performance(model1)

performance::model_performance(model2)

performance::model_performance(model3)


performance::check_model(model4)

performance::check_model(model5)

performance::check_model(model6)


performance::model_performance(model4)

performance::model_performance(model5)

performance::model_performance(model6)


```

$~$

## References: 

* https://rdrr.io/cran/MASS/man/dropterm.html
