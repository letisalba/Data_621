---
title: "Data 621 - Homework 3"
author: 'Group 2: William Aiken, Donald Butler, Michael Ippolito, Bharani Nittala,
  and Leticia Salazar'
date: "11-06-2022"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: yeti
    highlight: tango
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

$~$

## Overview: 
In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

$~$

## Objective: 
Build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). 

$~$

## Description:

Below is a short description of the variables of interest in the data set:

* zn: proportion of residential land zoned for large lots (over 25000 square feet)(predictor variable)
* indus: proportion of non-retail business acres per suburb (predictor variable) 
* chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
* nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)
* rm: average number of rooms per dwelling (predictor variable)
* age: proportion of owner-occupied units built prior to 1940 (predictor variable)
* dis: weighted mean of distances to five Boston employment centers (predictor variable)
* rad: index of accessibility to radical highways (predictor variable)
* tax: full-value property-tax rate per $10,000 (predictor variable)
* ptratio: pupil-teacher ratio by town (predictor variable)
* black: $1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town (predictor variable)
* lstat: lower status of the population (percent)(predictor variable)
* medv: median value of owner-occupied homes in $1000s (predictor variable)
* target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

-------------------------

$~$

### Load Libraries:

These are the libraries used to explore, prepare, analyze and build our models
```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(pROC)
library(corrplot)
library(GGally)
library(psych)
library(car)
library(kableExtra)
library(gridExtra)
library(performance)
library(faraway)
```


-------------------------

$~$

### Load Data set:

We have included the original data sets in our GitHub account and read from this location. Our data set includes 466 records and 13 variables.
```{r, loading data, echo=FALSE}
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_3/csv/crime-training-data_modified.csv")
glimpse(dftrain)

  
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_3/csv/crime-evaluation-data_modified.csv")
```


-------------------------

$~$

## Data Exploration:

The correlation plot below is measuring the degree of linear relationship within the training data set. The values in which this is measured falls between -1 and +1, with +1 being a stronger correlation.

```{r corrplot train, echo=FALSE}
corrplot(cor(dftrain, use = 'complete.obs'), tl.cex = 0.5)
```

$~$

To give more insight on our data set we used the `summary()` and `describe()` functions below: 
```{r fig.width=12, fig.height=12}
# summarizing data set
summary(dftrain)

describe(dftrain)
```

$~$

Factor categorical variables
```{r}
# from the training data set; variable: target
dftrain$target <- factor(dftrain$target, levels=c(0, 1))
levels(dftrain$target) <- list(below_median=0, above_median=1)

# from the training data set; variable: chas
dftrain$chas <- factor(dftrain$chas)
levels(dftrain$chas) <- list(not_on_charles=0, on_charles=1)

# from the evaluation data set; variable: chas
dfeval$chas <- factor(dfeval$chas)
levels(dfeval$chas) <- list(not_on_charles=0, on_charles=1)
```

$~$

The plot matrix below consists of scatter plots corresponding to each data frame
```{r, echo=FALSE, fig.width=12, fig.height=12}
pairs(dftrain)
```


$~$

These boxplots below are show plenty of variables in our training data set with outliers. We also notice that variables `rad` and `tax` have a higher median for crime rate.
```{r,echo=FALSE, fig.width=12, fig.height=12}
# Boxplots
par(mfrow=c(4, 3))
boxplot(zn ~ target, data=dftrain)
boxplot(indus ~ target, data=dftrain)
#boxplot(chas ~ target, data=dftrain) # excluding chas
boxplot(nox ~ target, data=dftrain)
boxplot(rm ~ target, data=dftrain)
boxplot(age ~ target, data=dftrain)
boxplot(dis ~ target, data=dftrain)
boxplot(rad ~ target, data=dftrain)
boxplot(tax ~ target, data=dftrain)
boxplot(ptratio ~ target, data=dftrain)
boxplot(lstat ~ target, data=dftrain)
boxplot(medv ~ target, data=dftrain)
```


$~$

We created a contingency table to show the distribution of a variables `target` and `chas`. By using a jitter plot we are trying to visualize the relationship between these two variables.
```{r,echo=FALSE}
# Contingency table 
dfconting <- data.frame(target=dftrain$target, chas=dftrain$chas)
table(dfconting)

# jittered plot for chas variable
plot(jitter(as.numeric(dftrain$chas), amount=.15), jitter(as.numeric(dftrain$target), amount=0.03), 
     xlab="Borders Charles River (2=yes)", ylab="Crime Rate > Median (2=yes)", col=dftrain$chas, pch=as.numeric(dftrain$chas))
```


Relationship of median crime rate to the following predictor variables:

| Predictor | Definition                                                                    | Relationship to Median Crime Rate |
|-----------|:-----------------------------------------------------------------------------:|:---------------------------------:|
| zn        | Proportion of residential land zoned for large lots (over 25000 square feet)  | negative                          |
| indus     | Proportion of non-retail business acres per suburb                            | positive                          |
| chas      | Dummy var. for whether the suburb borders the Charles River                   | unclear                           |
| nox       | Nitrogen oxides concentration                                                 | positive                          |
| rm        | Average number of rooms per dwelling                                          | unclear                           |
| age       | Proportion of owner-occupied units built prior to 1940                        | positive                          |
| dis       | Weighted mean of distances to five Boston employment centers                  | negative                          |
| rad       | Index of accessibility to radial highways                                     | positive                          |
| tax       | Full-value property-tax rate per $10,000                                      | positive                          |
| ptratio   | Pupil-teacher ratio by town                                                   | positive                          |
| lstat     | Lower status of the population (percent)                                      | positive                          |
| medv      | Median value of owner-occupied homes in $1000s                                | negative                          |

As indicated in the table, several predictors exhibit exhibite an inverse relationship with median crime rate. Based on the zn and medv variables, larger lot sizes and higher median home values correspond to a drop in crime rate, which is expected since larger lots and higher home values typically indicate higher economic status and, hence, lower crime. The same is true for the dis variable, which indicates that the farther a neighborhood is away from a major employment center, the lower the crime rate; this also makes sense, given that employment centers are often located in denser, more urban settings, which typically have higher rates of crime.

For the most part, variables exhibiting positive relationships with median crime rate also make intuitive sense. It would follow that neighborhoods having higher rates of industry (and, therefore, higher concentrations of pollutants like nitrogen oxides--nox--in the air) would also have higher crime rates. Likewise, neighborhoods with older homes (indicated by the age variable) located near radial highways (rad variable) and with a high pupil-to-teacher ratio (ptratio variable) could also be interpreted to have higher rates of crime.

Two variables didn't exhibit a clear relationships to crime rate: whether the neighborhood borders the Charles River (chas) and the average number of rooms per dwelling (rm). In addition, while the the lstat predictor exhibitied a positive relationship with crime rate, the description of the variable ("lower status of the population") didn't clearly state what the data values represent.

$~$

Now we'll look for any significant relationships among predictor variables. We considered correlation values above 0.6 to be significant.

Let's explore colinearity of predictor variables with the help of a correlation matrix:

```{r fig.width=12, fig.height=12, echo=FALSE}

# Correlation matrix
print('Correlation matrix (numerical variables):')
round(cor(dftrain[, c(1,2,4:12)]), 2)

# plots
par(mfrow=c(6, 3))

plot(zn ~ dis, data=dftrain)
abline(lm(zn ~ dis, data=dftrain), lt=2, col=2)

plot(indus ~ nox, data=dftrain)
abline(lm(indus ~ nox, dftrain), lt=2, col=2)

plot(indus ~ age, data=dftrain)
abline(lm(indus ~ age, dftrain), lt=2, col=2)

plot(indus ~ dis, data=dftrain)
abline(lm(indus ~ dis, dftrain), lt=2, col=2)

plot(indus ~ rad, data=dftrain)
abline(lm(indus ~ dis, dftrain), lt=2, col=2)

plot(indus ~ tax, data=dftrain)
abline(lm(indus ~ tax, dftrain), lt=2, col=2)

plot(indus ~ lstat, data=dftrain)
abline(lm(indus ~ lstat, dftrain), lt=2, col=2)

plot(nox ~ age, data=dftrain)
abline(lm(nox ~ age, dftrain), lt=2, col=2)

plot(nox ~ dis, data=dftrain)
abline(lm(nox ~ dis, dftrain), lt=2, col=2)

plot(nox ~ rad, data=dftrain)
abline(lm(nox ~ rad, dftrain), lt=2, col=2)

plot(nox ~ tax, data=dftrain)
abline(lm(nox ~ tax, dftrain), lt=2, col=2)

plot(nox ~ lstat, data=dftrain)
abline(lm(nox ~ lstat, dftrain), lt=2, col=2)

plot(rm ~ lstat, data=dftrain)
abline(lm(rm ~ lstat, dftrain), lt=2, col=2)

plot(rm ~ medv, data=dftrain)
abline(lm(rm ~ medv, dftrain), lt=2, col=2)

plot(age ~ dis, data=dftrain)
abline(lm(age ~ dis, dftrain), lt=2, col=2)

plot(age ~ lstat, data=dftrain)
abline(lm(age ~ lstat, dftrain), lt=2, col=2)

plot(rad ~ tax, data=dftrain)
abline(lm(rad ~ tax, dftrain), lt=2, col=2)

plot(lstat ~ medv, data=dftrain)
abline(lm(lstat ~ medv, dftrain), lt=2, col=2)
```

As shown in the graphs above, a number of significant correlations exist. Some of the stronger relationships are discussed here. First, the proportion of area zoned for large lots (zn) has a positive relationship with the distance to employment centers (dis), since it is more difficult to locate large lots close to the city center. A strong positive correlation exists between indus and nox, which is intuitively obvious. Likewise, tax rates in industrial areas are likely to be higher, as shown by the strong positive correlation of 0.73. Another strong correlation that makes obvious intuitive sense is that between median home values (medv) and the average number of rooms per dwelling (rm). The strongest positive correlation (0.91) exists between tax rate (tax) and the index of accessibility to radial highways (rad), which also corresponds to the fact that industrial areas are typically close to radial highways and also exhibit higher tax rates. The strongest negative correlation (-0.77) exists between nox and dis, indicating that the farther away from employment centers (and, hence, industrial areas), the lower the concentration of nitrogen oxide pollutants. Almost equally strong (-0.75) is the correlation between the age of dwellings (age) and the distance from employment centers (dis), indicating that the farther from urban centers, the newer the houses, which makes intuitive sense.

-------------------------

$~$

## Data Preparation:

There are no missing values for our data sets
```{r, echo=FALSE}
round(100*colSums(is.na(dftrain))/nrow(dftrain),2)
```

```{r, echo=FALSE}
round(100*colSums(is.na(dfeval))/nrow(dfeval),2)
```

The rad predictor is a categorical value and has some unknown meaning for values 1-8, 24. We need to introduce dummy variables rad1, rad2, etc to indicate if the neighborhood is in which category. We will exclude rad24 since we only need N-1 variables to represent each value.

```{r}
# cleaning train data 
clean_df <- function(df) {
  df$rad_1 <- ifelse(df$rad == 1, 1, 0)
  df$rad_2 <- ifelse(df$rad == 2, 1, 0)
  df$rad_3 <- ifelse(df$rad == 3, 1, 0)
  df$rad_4 <- ifelse(df$rad == 4, 1, 0)
  df$rad_5 <- ifelse(df$rad == 5, 1, 0)
  df$rad_6 <- ifelse(df$rad == 6, 1, 0)
  df$rad_7 <- ifelse(df$rad == 7, 1, 0)
  df$rad_8 <- ifelse(df$rad == 8, 1, 0)
  df$rad <- NULL
  return(df)
}

dftrain_clean <- clean_df(dftrain)
dftrain_clean <- dftrain_clean %>%
  select(target, everything())
dfeval_clean <- clean_df(dfeval)
```

-------------------------

$~$

## Model Building:

Logistic Regression: Ippolito
```{r}
# Maximal model for backward elimination
lmod.max <- glm(target ~ ., family=binomial(), data=dftrain)
summary(lmod.max)

# Backward elimination
lmod.back <- step(lmod.max, data=dftrain, direction='backward')
summary(lmod.back)

# Minimal model for forward elimination
lmod.min <- glm(target ~ 1, family=binomial(), data=dftrain)
summary(lmod.min)
lmod.max.form <- formula(lmod.max)

# Forward elimination
lmod.fwd <- step(lmod.min, data=dftrain, direction='forward', scope=lmod.max.form)
summary(lmod.fwd)
```


### Linear Model
Logistic Regression: Butler

#### You can't calculate residuals for a factor so I created a dummy target variable for this model

```{r model-lm, warning = FALSE, error = FALSE}
dftrain_clean_dummy <- dftrain_clean %>% mutate(target = as.numeric(target == "above_median"))
olsreg <- lm(data = dftrain_clean_dummy, formula = target ~ .)
summary(olsreg)
```



### Logit Model

```{r model_logit, warning=FALSE, message=FALSE}
logit <- glm(data = dftrain_clean, formula = target ~ ., family = binomial (link = "logit"))
summary(logit)
```


### Logit Model with Backward Elimination

```{r model_logit_back, warning=FALSE, message=FALSE}
# Backward elimination
lmod.back <- step(logit, data=dftrain_clean, direction='backward')
summary(lmod.back)
```


###  Logit Minimal Model with forward elimination

```{r model_minimal, warning=FALSE, message=FALSE}
# Minimal model for forward elimination
lmod.min <- glm(target ~ 1, family=binomial(), data=dftrain_clean)
summary(lmod.min)
```

```{r model_forward, warning=FALSE, message=FALSE}
# Forward elimination
lmod.fwd <- step(lmod.min, data=dftrain_clean, direction='forward', scope=formula(logit))
summary(lmod.fwd)
```





-------------------------

$~$

## Select Models: 

#### Here we are evaluating the performance our models to decide which model should we use


#### **Linear Model Accuracy**

```{r model_linear_accuracy, warning=FALSE, message=FALSE}
table(true = dftrain_clean_dummy$target, pred = round(fitted(olsreg)))
```

Accuracy: $\frac{210+225}{466}=93\%$

Classification Error Rate: $\frac{12+19}{466}=7\%$

Precision: $\frac{210}{210+12}=95\%$

Sensitivity: $\frac{210}{210+19}=92\%$

Specificity: $\frac{225}{225+12}=95\%$

F1 Score: $\frac{2 * .95 * .92}{.95 + .92}=93\%$


#### **Logit Model Prediction Accuracy**


```{r model_logit_accuracy, warning=FALSE, message=FALSE}
table(true = dftrain_clean$target, pred = round(fitted(logit)))
```

Accuracy: $\frac{219+233}{466}=97\%$

Classification Error Rate: $\frac{4+10}{466}=3\%$

Precision: $\frac{219}{219+4}=98\%$

Sensitivity: $\frac{219}{219+10}=96\%$

Specificity: $\frac{233}{233+4}=98\%$

F1 Score: $\frac{2 * .98 * .96}{.98 + .96}=97\%$


#### **Logit Model with Forward Elimination Prediction Accuracy**

```{r model_forward_accuracy, warning=FALSE, message=FALSE}
table(true = dftrain_clean$target, pred = round(fitted(lmod.fwd)))
```

Accuracy: $\frac{220+234}{466}=97\%$

Classification Error Rate: $\frac{5+12}{466}=3\%$

Precision: $\frac{220}{220+3}=99\%$

Sensitivity: $\frac{220}{220+9}=96\%$

Specificity: $\frac{234}{234+3}=98\%$

F1 Score: $\frac{2 * .99 * .96}{.99 + .96}=97\%$


#### **Logit Model with Backward Elimination Prediction Accuracy**

```{r model_logit_back_accuracy, warning=FALSE, message=FALSE}
table(true = dftrain_clean$target, pred = round(fitted(lmod.back)))
```

Accuracy: $\frac{217+232}{466}=96\%$

Classification Error Rate: $\frac{5+12}{466}=4\%$

Precision: $\frac{217}{217+5}=98\%$

Sensitivity: $\frac{217}{217+12}=95\%$

Specificity: $\frac{232}{232+5}=98\%$

F1 Score: $\frac{2 * .98 * .95}{.98 + .95}=96\%$

### Model AUCs

* Linear Model

```{r}

pred = round(fitted(olsreg))
pROC::auc(dftrain_clean_dummy$target, pred)

```

* Logit Model

```{r}
pred = round(fitted(logit))
pROC::auc(dftrain_clean$target, pred)

```


* Logit Model with Forward Elimination 

```{r}
pred = round(fitted(lmod.fwd))
pROC::auc(dftrain_clean$target, pred)

```

* Logit Model with Backward Elimination

```{r}
pred = round(fitted(lmod.back))
pROC::auc(dftrain_clean$target, pred)

```

#### Findings

All of our Logistic Models had a better AUC than our Linear Model with Logit with Forward Elimination having the best (0.974)

Now lets make predictions with the evaluation df.

* Linear Model

```{r}
prediction <- broom::augment(olsreg, newdata = dfeval_clean)
prediction$.fitted
```
* Logit Model

```{r}
prediction <- broom::augment(logit, newdata = dfeval_clean)
prediction$.fitted
```

* Logit Model with Forward Elimination 

```{r}
prediction <- broom::augment(lmod.fwd, newdata = dfeval_clean)
prediction$.fitted
```

* Logit Model with Backward Elimination

```{r}
prediction <- broom::augment(lmod.back, newdata = dfeval_clean)
prediction$.fitted
```





-------------------------

$~$

## Appendix:

```{r}
# Code used
```


