<<<<<<< HEAD
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
=======
selectInput(inputId = 'y','Selected ICD.Chapter:',
choices = infec_diseases$ICD.Chapter,
selected = 'State')
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotlyOutput('distPlot', height = "700px", width = "600px")
)
)
)
library(shiny)
library(tidyverse)
library(ggplot2)
library(rsconnect)
library(plotly)
data <- read.csv("https://raw.githubusercontent.com/charleyferrari/CUNY_DATA_608/master/module3/data/cleaned-cdc-mortality-1999-2010-2.csv", header = TRUE)
str(data)
summary(data)
colnames(data)
colSums(is.na(data))
rsconnect::setAccountInfo(name='letisalba', token='18C2DF783293E23721636AD4A43F97C6', secret='seSfGaZ7eiF653BaFtyAGHFls/Z66OE/t7Sk2b9A')
infec_diseases <- filter(data, Year == '2010' & ICD.Chapter == 'Certain infectious and parasitic diseases')
infec_diseases <- infec_diseases %>%
arrange(Crude.Rate)
head(infec_diseases)
# plot
infec_diseases %>%
ggplot(aes(x = reorder(State, -Crude.Rate), y = Crude.Rate)) +
geom_bar(stat = "identity") +
theme(axis.text = element_text(size = 6), panel.background = element_rect(fill = "white")) +
geom_col(fill = "blue")+
coord_flip() +
xlab("State") +
ylab("Crude Rate") +
geom_text(aes(label = Crude.Rate), vjust = 0.6, hjust = -0.4, size = 2, color="black")
fig <- infec_diseases %>% plot_ly()
fig <- fig %>% add_trace(x = ~Crude.Rate, y = ~State, type = 'bar',
text = y, textposition = 'auto',
marker = list(color = 'rgb(158,202,225)',
line = list(color = 'rgb(8,48,107)', width = 1.5)))
# Define UI for app that draws a histogram ----
ui <- fluidPage(
# App title ----
titlePanel("Crude Mortality Rate"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
selectInput(inputId = 'y','Selected ICD.Chapter:',
choices = infec_diseases$ICD.Chapter,
selected = 'State')
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotlyOutput('distPlot', height = "700px", width = "600px")
)
)
)
# Define server logic required to draw a bargraph
server <- function(input, output) {
output$distPlot <- renderPlotly({fig})
}
# Run the application
shinyApp(ui, server)
# Define UI for app that draws a histogram ----
ui <- fluidPage(
# App title ----
titlePanel("Crude Mortality Rate"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
selectInput(inputId = 'y','Selected ICD.Chapter:',
choices = infec_diseases$ICD.Chapter,
selected = 'State')
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotlyOutput('distPlot', height = "700px", width = "600px")
)
)
)
# Define server logic required to draw a bargraph
server <- function(input, output) {
output$distPlot <- renderPlotly({fig})
}
# Run the application
shinyApp(ui, server)
infec_diseases_2 <- data %>%
group_by(Year == "2010", ICD.Chapter == "Certain infectious and parasitic diseases") %>%
mutate(Crude.Rate.2 = round(
sum(Deaths) / sum(Population) * 100000),3) %>%
group_by(Year, ICD.Chapter, State)
head(infec_diseases_2)
state = "CA"
fig_2<- infec_diseases_2 %>%
plot_ly(x = ~Year, y = ~Crude.Rate, type ='bar',
text = y, textposition = 'auto',
marker = list(color = 'rgb(158,202,225)'),
name = 'State') %>% # Chart State and US Crude Rates next to one another
add_trace(x = ~Year, y = ~Crude.Rate.2, type='bar',
text = y, textposition = 'auto',
marker = list(color = 'rgb(58,200,225)'), name = 'US')  %>%
layout(title = "Crude death rate by state [CA]",
barmode = 'group',
xaxis = list(title = "Year"),
yaxis = list(title = "Crude death rate"))
# Define UI for app that draws a histogram ----
ui <- fluidPage(
# App title ----
titlePanel("Crude Mortality Rate"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
selectInput(inputId = 'y','Selected ICD.Chapter:',
choices = infec_diseases_2$ICD.Chapter,
selected = 'State')
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Create barchart
plotlyOutput('distPlot2')
)
)
)
server <- function(input, output) {
output$distPlot2 <- renderPlotly({fig_2})
}
shinyApp(ui, server)
rsconnect::deployApp('/Users/letiix3/Desktop/DATA_608/module3')
>>>>>>> 4dc3cf2f2defe5f0d5afa3093b5e71afe374bef7
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
methods(filter)
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% stats::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% stats::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
install.packages("BiocManager")
BiocManager::valid()
BiocManager::install(version = "3.15")
install.packages('devtools')
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow({df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1}))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Showing this empirically
# Create values of precision and sensitivity
n <- 100
prec <- c()
sens <- c()
for (i in seq(1, 1000)) {
TP <- sample(seq(0, 100), size=1)
FP <- sample(seq(0, n - TP), size=1)
FN <- sample(seq(0, n - TP - FP), size=1)
TN <- 100 - TP - FP - FN
tmp_prec = TP / (TP + FP)
tmp_sens = TP / (TP + FN)
if (TP + TN + FP + FN != 100 | (TP + FP == 0 & TP + FN == 0)) {
print(paste0('TP=', TP, ', TN=', TN, ', FP=', FP, ', FN=', FN, ', n=', TP + TN + FP + FN))
}
prec <- c(prec, tmp_prec)
sens <- c(sens, tmp_sens)
}
plot(prec, sens)
f1 <- (2*prec*sens)/(prec + sens)
boxplot(f1, main='F1 scores')
plot(f1)
# Show that as precision approaches 0 while sensitivity is close to zero, the f1 score approaches zero.
p <- seq(0, 1, 0.0001)
s <- rep(0.001, length(p))
f1 <- (2 * p * s) / (p + s)
plot(p, f1)
roc_func <- function(data){
temp_x <- rep(0, 101)
temp_y <- rep(0, 101)
temp_seq <- seq(from = 0, to = 1, by = 0.01)
for (i in 1:length(temp_seq)){
df <- data %>% mutate(scored.class = as.numeric(scored.probability > temp_seq[i])) %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0))
TPR = sum(df$TP)/(sum(df$TP) + sum(df$FN))
FPR = sum(df$FP)/(sum(df$FP) + sum(df$TN))
temp_x[i] <- FPR
temp_y[i] <- TPR
}
temp_df <- bind_cols(temp_x, temp_y) %>% as.data.frame()
names(temp_df) <- c("FPR", "TPR")
plt <- ggplot2::ggplot(data = temp_df, aes(x = FPR, y = TPR)) + geom_point() + geom_abline()
AUC <- pracma::trapz(temp_x,temp_y)
output <- list(plt, AUC)
return(output)
}
roc_func(class_df)
roc_func <- function(data){
temp_x <- rep(0, 101)
temp_y <- rep(0, 101)
temp_seq <- seq(from = 0, to = 1, by = 0.01)
for (i in 1:length(temp_seq)){
df <- data %>% mutate(scored.class = as.numeric(scored.probability > temp_seq[i])) %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0))
TPR = sum(df$TP)/(sum(df$TP) + sum(df$FN))
FPR = sum(df$FP)/(sum(df$FP) + sum(df$TN))
temp_x[i] <- FPR
temp_y[i] <- TPR
}
temp_df <- bind_cols(temp_x, temp_y) %>% as.data.frame()
names(temp_df) <- c("FPR", "TPR")
plt <- ggplot2::ggplot(data = temp_df, aes(x = FPR, y = TPR)) + geom_point() + geom_abline()
AUC <- pracma::trapz(temp_x,temp_y)
output <- list(plt, AUC)
return(output)
}
roc_func(class_df)
round(func.accuracy(class_df,"class","scored.class"),5)
error_rate <- func.error_rate(class_df)
error_rate <- func.error_rate(class_df)
round(func.precision(class_df),5)
round(func.sensitivity(class_df),5)
round(func.specificity(class_df),5)
calcF1(df, 'class', 'scored.class')
roc_func(class_df)
func.error_rate(class_df)
caret <- confusionMatrix(as.factor(class_df$scored.class), as.factor(class_df$class), positive = "1")
caret
Sensitivity == caret$byClass["Sensitivity"]
Specificity == caret$byClass["Specificity"]
Accuracy == caret$overall["Accuracy"]
round(func.error_rate(class_df),5)
Class_Error_Rate <- round(func.error_rate(class_df),5)
Class_Error_Rate
Sensitivity <- round(func.sensitivity(class_df),5)
Sensitivity
Specificity <- round(func.specificity(class_df),5)
Specificity
F1_Score <- round(calcF1(df, 'class', 'scored.class'),5)
Sensitivity == caret$byClass["Sensitivity"]
Specificity == caret$byClass["Specificity"]
Accuracy == caret$overall["Accuracy"]
Accuracy <- round(func.accuracy(class_df,"class","scored.class"),5)
Accuracy
Accuracy == caret$overall["Accuracy"]
par(mfrow = c(1,2))
roc <- plot(roc(class_df$class, class_df$scored.probability), print.auc = TRUE, main = "ROC Curve from pROC Package")
roc
data_1 <- data %>%
select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
data_1 <- data %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(b$scored.class, b$class, positive = "1")
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(b$scored.class, data_1$class, positive = "1")
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accurary_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accuracy_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accuracy_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(class_df), Sensitivity(class_df), Specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(class_df), Sensitivity(class_df), Specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), func.sensitivity(data), func.specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
dplyr::mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), func.sensitivity(data), func.specificity(data))
cm <- class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
value2 <- c(cm$overall[1], 1 - cm$overall[1], cm$byClass[3],
cm$byClass[1], cm$byClass[2], cm$byClass[7])
Accuracy == round(caret$overall["Accuracy"],5)
Sensitivity == round(caret$byClass["Sensitivity"],5)
Specificity == round(caret$byClass["Specificity"],5)
par(mfrow = c(1,1))
roc <- plot(roc(class_df$class, class_df$scored.probability), print.auc = TRUE, main = "ROC Curve from pROC Package")
roc
caret <- confusionMatrix(as.factor(class_df$scored.class), as.factor(class_df$class), positive = "1")
caret
Accuracy == round(caret$overall["Accuracy"],5)
Sensitivity == round(caret$byClass["Sensitivity"],5)
Specificity == round(caret$byClass["Specificity"],5)
Accuracy <- round(func.accuracy(class_df,"class","scored.class"),5)
Accuracy
class_df %>%
select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
<<<<<<< HEAD
=======
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(GGally)
library(psych)
library(car)
library(kableExtra)
library(gridExtra)
# 1
# Load data
df <- read.csv('https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv')
# Summary stats
summary(df)
# 2
# Confusion matrix
(cf <- table(df$class, df$scored.class))
# rows contain the actual values, columns are the predicted values
# TN FP
# FN TP
# 3
calcAccuracy <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
TN <- nrow(df %>% filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 0))
n <- nrow(df)
return((TP + TN) / n)
}
calcAccuracy(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(class_df, 'class', 'scored.class')
# Calculate sensitivity using dynamic column names
calcSensitivity <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FN <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 0))
return(TP / (TP + FN))
}
calcSensitivity(class_df, 'class', 'scored.class')
# Calculate F1 score
calcF1 <- function(df, actualCol, predictedCol) {
tmp_precision <- calcPrecision(df, actualCol, predictedCol)
tmp_sensitivity <- calcSensitivity(df, actualCol, predictedCol)
return((2 * tmp_precision * tmp_sensitivity) / (tmp_precision + tmp_sensitivity))
}
calcF1(class_df, 'class', 'scored.class')
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
>>>>>>> 4dc3cf2f2defe5f0d5afa3093b5e71afe374bef7
