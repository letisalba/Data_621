}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
methods(filter)
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% stats::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% stats::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
install.packages("BiocManager")
BiocManager::valid()
BiocManager::install(version = "3.15")
install.packages('devtools')
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pROC)
library(caret)
library(tidyverse)
library(dplyr)
library(conflicted)
# load data set
class_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_2/csv/classification-output-data.csv")
head(class_df, n = 4)
summary(class_df)
str(class_df)
class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
func.accuracy <- function (class_df, actual, predict) {
accuracy <- sum(class_df[actual] == class_df[predict]) / nrow(class_df)
return (accuracy)
}
paste0('The accuracy of the predictions is ', round(func.accuracy(class_df,"class","scored.class"),5))
func.error_rate <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
TN = sum(df$TN)
FP = sum(df$FP)
FN = sum(df$FN)
return((FP+FN)/(TP+FP+TN+FN))
}
error_rate <- func.error_rate(class_df)
#accuracy <- func.accuracy(class_df)
accuracy <- func.accuracy(class_df,"class","scored.class")
# Verify that you get an accuracy and an error rate that sums to one.
paste0('The accuracy and error rate sums to ', (accuracy + error_rate))
func.precision <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TP = sum(df$TP)
FP = sum(df$FP)
return(TP/(TP+FP))
}
paste0('The precision of the predictions is ',round(func.precision(class_df),5))
func.sensitivity <- function(data) {
df <- data %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0))
TP = sum(df$TP)
FN = sum(df$FN)
return(TP/(TP+FN))
}
paste0('The sensitivity of the predictions is ', round(func.sensitivity(class_df),5))
func.specificity <- function(data) {
df <- data %>%
mutate(TN = ifelse(class == 0 & scored.class == 0,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0))
TN = sum(df$TN)
FP = sum(df$FP)
return(TN/(TN+FP))
}
paste0('The specificity of the predictions is ', round(func.specificity(class_df),5))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow({df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1}))
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Calculate precision using dynamic column names
calcPrecision <- function(df, actualCol, predictedCol) {
TP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 1 & (!!sym(predictedCol)) == 1))
FP <- nrow(df %>% dplyr::filter((!!sym(actualCol)) == 0 & (!!sym(predictedCol)) == 1))
return(TP / (TP + FP))
}
calcPrecision(df, 'class', 'scored.class')
# Showing this empirically
# Create values of precision and sensitivity
n <- 100
prec <- c()
sens <- c()
for (i in seq(1, 1000)) {
TP <- sample(seq(0, 100), size=1)
FP <- sample(seq(0, n - TP), size=1)
FN <- sample(seq(0, n - TP - FP), size=1)
TN <- 100 - TP - FP - FN
tmp_prec = TP / (TP + FP)
tmp_sens = TP / (TP + FN)
if (TP + TN + FP + FN != 100 | (TP + FP == 0 & TP + FN == 0)) {
print(paste0('TP=', TP, ', TN=', TN, ', FP=', FP, ', FN=', FN, ', n=', TP + TN + FP + FN))
}
prec <- c(prec, tmp_prec)
sens <- c(sens, tmp_sens)
}
plot(prec, sens)
f1 <- (2*prec*sens)/(prec + sens)
boxplot(f1, main='F1 scores')
plot(f1)
# Show that as precision approaches 0 while sensitivity is close to zero, the f1 score approaches zero.
p <- seq(0, 1, 0.0001)
s <- rep(0.001, length(p))
f1 <- (2 * p * s) / (p + s)
plot(p, f1)
roc_func <- function(data){
temp_x <- rep(0, 101)
temp_y <- rep(0, 101)
temp_seq <- seq(from = 0, to = 1, by = 0.01)
for (i in 1:length(temp_seq)){
df <- data %>% mutate(scored.class = as.numeric(scored.probability > temp_seq[i])) %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0))
TPR = sum(df$TP)/(sum(df$TP) + sum(df$FN))
FPR = sum(df$FP)/(sum(df$FP) + sum(df$TN))
temp_x[i] <- FPR
temp_y[i] <- TPR
}
temp_df <- bind_cols(temp_x, temp_y) %>% as.data.frame()
names(temp_df) <- c("FPR", "TPR")
plt <- ggplot2::ggplot(data = temp_df, aes(x = FPR, y = TPR)) + geom_point() + geom_abline()
AUC <- pracma::trapz(temp_x,temp_y)
output <- list(plt, AUC)
return(output)
}
roc_func(class_df)
roc_func <- function(data){
temp_x <- rep(0, 101)
temp_y <- rep(0, 101)
temp_seq <- seq(from = 0, to = 1, by = 0.01)
for (i in 1:length(temp_seq)){
df <- data %>% mutate(scored.class = as.numeric(scored.probability > temp_seq[i])) %>%
mutate(TP = ifelse(class == 1 & scored.class == 1,1,0),
FP = ifelse(class == 0 & scored.class == 1,1,0),
FN = ifelse(class == 1 & scored.class == 0,1,0),
TN = ifelse(class == 0 & scored.class == 0,1,0))
TPR = sum(df$TP)/(sum(df$TP) + sum(df$FN))
FPR = sum(df$FP)/(sum(df$FP) + sum(df$TN))
temp_x[i] <- FPR
temp_y[i] <- TPR
}
temp_df <- bind_cols(temp_x, temp_y) %>% as.data.frame()
names(temp_df) <- c("FPR", "TPR")
plt <- ggplot2::ggplot(data = temp_df, aes(x = FPR, y = TPR)) + geom_point() + geom_abline()
AUC <- pracma::trapz(temp_x,temp_y)
output <- list(plt, AUC)
return(output)
}
roc_func(class_df)
round(func.accuracy(class_df,"class","scored.class"),5)
error_rate <- func.error_rate(class_df)
error_rate <- func.error_rate(class_df)
round(func.precision(class_df),5)
round(func.sensitivity(class_df),5)
round(func.specificity(class_df),5)
calcF1(df, 'class', 'scored.class')
roc_func(class_df)
func.error_rate(class_df)
caret <- confusionMatrix(as.factor(class_df$scored.class), as.factor(class_df$class), positive = "1")
caret
Sensitivity == caret$byClass["Sensitivity"]
Specificity == caret$byClass["Specificity"]
Accuracy == caret$overall["Accuracy"]
round(func.error_rate(class_df),5)
Class_Error_Rate <- round(func.error_rate(class_df),5)
Class_Error_Rate
Sensitivity <- round(func.sensitivity(class_df),5)
Sensitivity
Specificity <- round(func.specificity(class_df),5)
Specificity
F1_Score <- round(calcF1(df, 'class', 'scored.class'),5)
Sensitivity == caret$byClass["Sensitivity"]
Specificity == caret$byClass["Specificity"]
Accuracy == caret$overall["Accuracy"]
Accuracy <- round(func.accuracy(class_df,"class","scored.class"),5)
Accuracy
Accuracy == caret$overall["Accuracy"]
par(mfrow = c(1,2))
roc <- plot(roc(class_df$class, class_df$scored.probability), print.auc = TRUE, main = "ROC Curve from pROC Package")
roc
data_1 <- data %>%
select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
data_1 <- data %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(b$scored.class, b$class, positive = "1")
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(b$scored.class, data_1$class, positive = "1")
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accurary_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accuracy_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(accuracy_predictions(class_df), sensitivity(class_df), specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(class_df), Sensitivity(class_df), Specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(class_df), Sensitivity(class_df), Specificity(class_df))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(Accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), Sensitivity(data), Specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), func.sensitivity(data), func.specificity(data))
data_1 <- class_df %>%
dplyr::select(scored.class, class) %>%
dplyr::mutate(scored.class = as.factor(scored.class),
class = as.factor(class))
Conf_Mat <- confusionMatrix(data_1$scored.class, data_1$class, positive = "1")
caret_package <- c(Conf_Mat$overall["Accuracy"], Conf_Mat$byClass["Sensitivity"], Conf_Mat$byClass["Specificity"])
written_function <- c(func.accuracy(data), func.sensitivity(data), func.specificity(data))
cm <- class_df %>%
dplyr::select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
value2 <- c(cm$overall[1], 1 - cm$overall[1], cm$byClass[3],
cm$byClass[1], cm$byClass[2], cm$byClass[7])
Accuracy == round(caret$overall["Accuracy"],5)
Sensitivity == round(caret$byClass["Sensitivity"],5)
Specificity == round(caret$byClass["Specificity"],5)
par(mfrow = c(1,1))
roc <- plot(roc(class_df$class, class_df$scored.probability), print.auc = TRUE, main = "ROC Curve from pROC Package")
roc
caret <- confusionMatrix(as.factor(class_df$scored.class), as.factor(class_df$class), positive = "1")
caret
Accuracy == round(caret$overall["Accuracy"],5)
Sensitivity == round(caret$byClass["Sensitivity"],5)
Specificity == round(caret$byClass["Specificity"],5)
Accuracy <- round(func.accuracy(class_df,"class","scored.class"),5)
Accuracy
class_df %>%
select(class, scored.class) %>%
# re-coding to label the 0's and 1's
mutate(class = recode(class,
'0' = 'Actual Negative',
'1' = 'Actual Positive'),
scored.class = recode(scored.class,
'0' = 'Predicted Negative',
'1' = 'Predicted Positive')) %>%
table()
