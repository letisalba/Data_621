```{r setup, include=FALSE, eval=FALSE}

knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)

# Load Libraries:
library(tidyverse)
library(dplyr)
library(corrplot)
library(skimr)
library(DataExplorer)
library(ggplot2)
library(hrbrthemes)
library(mice)
library(MASS)
library(dvmisc)
library(gridExtra)
library(lattice)
library(faraway)
library(pscl)

# Load Data set:
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-training-data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-evaluation-data.csv")
head(dftrain)
summary(dftrain)
summary(dfeval)
DataExplorer::create_report(dftrain, output_file='training_report.html')
DataExplorer::create_report(dfeval %>% dplyr::select(-TARGET), output_file='eval_report.html')
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
  gather(key = 'variable', value = 'value')
ggplot(plot_train) +
  geom_histogram(aes(x=value, y = ..density..), bins=30) +
  geom_density(aes(x=value), color='blue') +
  theme_ipsum() +
  facet_wrap(. ~variable, scales='free', ncol=4)

# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
    mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
    mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))

# Look at cases sold vs predictors
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(3, 16)) {  # skip INDEX and TARGET variables
    if (class(dftrain[, i]) == 'numeric') {
        tmpmin <- min(dftrain[, i], na.rm=T)
        tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
        tmpcuts <- c()
        for (j in seq(1, 5)) {
            tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
        }
        #dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
        dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
    } else {
        dftrain$x <- dftrain[, i]
    }
    dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
    plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[3:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)

# Remove index columns
dftrain <- dftrain %>% 
  dplyr::select(-INDEX)
dfeval <- dfeval %>% 
  dplyr::select(-IN)

# Impute missing values in training data (except for STARS)
dftrain_imputed <- mice(dftrain %>% dplyr::select(-STARS), m=5, maxit=5, method='pmm')
cleandf <- complete(dftrain_imputed) %>%
    mutate(STARS = dftrain$STARS)

# Impute missing values in eval data (except for STARS and TARGET)
dfeval_imputed <- mice(dfeval %>% dplyr::select(-STARS, -TARGET), m=5, maxit=5, method='pmm')
cleandf_eval <- complete(dfeval_imputed) %>%
    mutate(STARS = dfeval$STARS, TARGET = dfeval$TARGET)

# Look again at summary
summary(cleandf)
summary(cleandf_eval)

# Possion Model 1
p_mod1 <- glm(TARGET ~., family="poisson", data=cleandf)
summary(p_mod1)

# Possion Model with stepwise AIC approach
p_mod2 <- stepAIC(p_mod1, trace = F)
summary(p_mod2)

# MLR Model 1
lm_mod1 <- lm(TARGET ~., data = cleandf)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)

# MLR Model 2
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)

# NB model 1
nb_mod1 <- glm.nb(TARGET ~., data=cleandf)
aic_nb_mod1 = AIC(nb_mod1)
summary(nb_mod1)

# NB model 2
nb_mod2 <- stepAIC(nb_mod1, trace = F)
aic_nb_mod2 = AIC(nb_mod2)
summary(nb_mod2)

# Zero-inflated Poisson Model
zi_mod1 <- zeroinfl(TARGET ~ ., data=cleandf)
summary(zi_mod1)
aic_zi_mod1 = AIC(zi_mod1)
zi_mod2 <- stepAIC(zi_mod1, trace=F)  # this takes a long time to run
summary(zi_mod2)
aic_zi_mod2 = AIC(zi_mod2)

# Poisson Model 1:
aic_p_mod1 <- p_mod1$aic
aic_p_mod1

# Poisson Model 2:
aic_p_mod2 <- p_mod2$aic
aic_p_mod2

# Poisson - Minimum AIC
c(p_mod1$formula,p_mod2$formula)[which.min(c(p_mod1$aic,p_mod2$aic))]

#Linear Model 1:
r2_lm_mod1 <- summary(lm_mod1)$adj.r.squared
r2_lm_mod1

#Linear Model 2:
r2_lm_mod2 <- summary(lm_mod2)$adj.r.squared
r2_lm_mod2

# Multiple Linear Regression Model - Highest Adjusted R Squared
c(formula(lm_mod1),formula(lm_mod2))[which.max(c(summary(lm_mod1)$adj.r.squared, summary(lm_mod2)$adj.r.squared))]

#NB Model 1:
aic_nb_mod1

#NB Model 2:
aic_nb_mod2

#ZI Model 1:
aic_zi_mod1

#ZI Model 2:
aic_zi_mod2

# Mean Square Error:
mse <- function(df, model){
  mean((df$TARGET - predict(model))^2)
}
mse_p_mod1 <- mse(cleandf, p_mod1)
mse_p_mod2 <- mse(cleandf, p_mod2)
mse_lm_mod1 <- get_mse(lm_mod1)
mse_lm_mod2 <- get_mse(lm_mod2)
mse_nb_mod1 <- mse(cleandf, nb_mod1)
mse_nb_mod2 <- mse(cleandf, nb_mod2)
mse_zi_mod1 <- mse(cleandf, zi_mod1)
mse_zi_mod2 <- mse(cleandf, zi_mod2)

# Comparison of Models:
models <- c("Possion Model 1", "Possion Model 2", "Linear Model 1", "Linear Model 2", 
            "Neg Binom Model 1", "Neg Binom Model 2", "Zero-Infl Model 1", "Zero-Infl Model 2")
MSE <- round(c(mse_p_mod1, mse_p_mod2, mse_lm_mod1, mse_lm_mod2, mse_nb_mod1, mse_nb_mod2, mse_zi_mod1, mse_zi_mod2), 1)
AIC <- round(c(aic_p_mod1, aic_p_mod2, aic_lm_mod1, aic_lm_mod2, aic_nb_mod1, aic_nb_mod2, aic_zi_mod1, aic_zi_mod2), 1)
knitr::kable(rbind(MSE, AIC), col.names = models)

# LM model evaluation

# Load libraries
library(car)
library(lmtest)
library(olsrr)

# Define function to calculate mean squared error
calc_mse <- function(lmod) {
  return(mean((summary(lmod))$residuals ^ 2))
}

# Define function to aid in model analysis
ModelAnalysis <- function(lmod) {

  # Plot residuals
  print('--------------------------------------------------')
  print(lmod$call)
  par(mfrow=c(2,2))
  plot(lmod)
  print('')

  # Shapiro test to determine normality of residuals
  # Null hypothesis: the residuals are normal.
  # If the p-value is small, reject the null, i.e., consider the residuals *not* normally distributed.
  if (length(lmod$fitted.values) > 3 & length(lmod$fitted.values) < 5000) {
      st <- shapiro.test(lmod$residuals)
      if (st$p.value <= 0.05) {
        print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is <= 0.05, so reject the null; i.e., the residuals are NOT NORMAL"))
      } else {
        print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is > 0.05, so do not reject the null; i.e., the residuals are NORMAL"))
      }
      print('')
  } else {
      print("Shapiro test for normality of residuals cannot be performed; sample length must be between 3 and 5000.")
  }
     
  # Breusch-Pagan test to determine homoschedasticity of residuals
  # Null hypothesis: the residuals are homoschedastic.
  # If the p-value is small, reject the null, i.e., consider the residuals heteroschedastic.
  bp <- bptest(lmod)
  if (bp$p.value > 0.05 & bp$statistic < 10) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is > 0.05 and the test statistic of ", bp$statistic,
          " is < 10, so don't reject the null; i.e., the residuals are HOMOSCHEDASTIC"))
  } else if (bp$p.value <= 0.05) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is <= 0.05 and the test statistic is ", bp$statistic,
          ", so reject the null; i.e., the residuals are HETEROSCHEDASTIC"))
  } else {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " and test statistic of ", bp$statistic,
          " are inconclusive, so homoschedasticity can't be determined using this test."))
  }
  print('')

  # Visually look for colinearity - dont do this for large models
  #pairs(model.matrix(lmod))

  # Variance inflation factor (VIF)
  print('Variance inflation factor (VIF)')
  print('<=1: not correlated, 1-5: moderately correlated, >5: strongly correlated')
  print(sort(vif(lmod), decreasing=T))
  print('')
  
  # Standardized residual plots (look for points outside of 2 or 3 stdev)
  p <- length(summary(lmod)$coeff[,1] - 1)  # number of model parameters
  stanres <- rstandard(lmod)
  for (i in seq(1, ceiling(p / 4))) {
    par(mfrow=c(2,2))
    starti <- ((i - 1) * 4) + 1
    for (j in seq(starti, starti + 3)) {
      if (j + 1 <= ncol(model.matrix(lmod))) {
        # Skip these plots since we're pretty sure that a linear model isn't valid here
        #plot(model.matrix(lmod)[, j + 1], stanres, xlab=colnames(model.matrix(lmod))[j + 1], ylab='Standardized residuals')
        #abline(h=c(-2, 2), lt=3, col='blue')
        #abline(h=c(-3, 3), lt=2, col='red')
      }
    }
  }
  
  # Model scores
  print('Model scores:')
  print(paste0('    adjusted R-squared: ', round(summary(lmod)$adj.r.squared, 3)))
  print(paste0('    AIC: ', round(AIC(lmod, k=2), 3)))
  print(paste0('    BIC: ', round(BIC(lmod), 3)))
  print(paste0('    Mallow\'s Cp: ', round(ols_mallows_cp(lmod, fullmodel=lmod), 3)))
  print(paste0('    mean squared error: ', round(calc_mse(lmod), 3)))
  print('')
  
  # Find leverage point cutoff
  n <- length(lmod$residuals)
  cutoff <- 2 * (p + 1) / n
  print(paste0('Leverage point cutoff: ', cutoff))
  print('')

  # Show points of influence
  print('First 10 points of influence:')
  poi <- lm.influence(lmod)$hat
  len_poi <- length(poi)
  ct <- 0
  for (i in seq(1, length(poi))) {
    if (poi[i] > cutoff) {
      ct <- ct + 1
      print(paste0('    case #', i, ': ', round(poi[i], 3)))
    }
        if (ct > 10) {
            break
        }
  }
  print('')
  
}
ModelAnalysis(lm_mod1)
ModelAnalysis(lm_mod2)

# Calc overdispersion
op1 <- p_mod1$deviance / p_mod1$df.residual
op2 <- p_mod2$deviance / p_mod2$df.residual
print(paste0('Poisson model 1 overdispersion: ', op1))
print(paste0('Poisson model 2 overdispersion: ', op2))

# Calc accuracies

# Show histogram of actual cases sold to compare with histograms generated by predictions
par(mfrow=c(2, 3))
hist(cleandfnew$TARGET, xlab='Cases', main='Actual Cases Sold')

# Gaussian
cleandfnew$pred_target <- predict(lm_mod2, cleandfnew, interval ='prediction')
cleandfnew$correct <- ifelse(round(cleandfnew$pred_target, 0) == cleandfnew$TARGET, 1, 0)
acc_num <- sum(cleandfnew$correct)
acc_pct <- round(100 * acc_num / nrow(cleandfnew), 1)
print(paste0('Gaussian accuracy: ', acc_num, ' of ', nrow(cleandfnew), ' (', acc_pct, '%)'))
hist(round(cleandfnew$pred_target, 0), xlab='Cases', main='Predictions (Gaussian)')

# Negative binomial
cleandfnew <- cleandf
cleandfnew$pred_target <- exp(predict(nb_mod2, cleandfnew, interval ='prediction'))
cleandfnew$correct <- ifelse(round(cleandfnew$pred_target, 0) == cleandfnew$TARGET, 1, 0)
acc_num <- sum(cleandfnew$correct)
acc_pct <- round(100 * acc_num / nrow(cleandfnew), 1)
print(paste0('Negative binomial model accuracy: ', acc_num, ' of ', nrow(cleandfnew), ' (', acc_pct, '%)'))
hist(round(cleandfnew$pred_target, 0), xlab='Cases', main='Predictions (Negative Binomial)')

# Poisson
cleandfnew$pred_target <- exp(predict(p_mod2, cleandfnew, interval ='prediction'))
cleandfnew$correct <- ifelse(round(cleandfnew$pred_target, 0) == cleandfnew$TARGET, 1, 0)
acc_num <- sum(cleandfnew$correct)
acc_pct <- round(100 * acc_num / nrow(cleandfnew), 1)
print(paste0('Poisson accuracy: ', acc_num, ' of ', nrow(cleandfnew), ' (', acc_pct, '%)'))
hist(round(cleandfnew$pred_target, 0), xlab='Cases', main='Predictions (Poisson)')

# Zero-inflated
cleandfnew$pred_target <- predict(zi_mod2, cleandfnew, interval ='prediction')
cleandfnew$correct <- ifelse(round(cleandfnew$pred_target, 0) == cleandfnew$TARGET, 1, 0)
acc_num <- sum(cleandfnew$correct)
acc_pct <- round(100 * acc_num / nrow(cleandfnew), 1)
print(paste0('Zero-inflated poisson accuracy: ', acc_num, ' of ', nrow(cleandfnew), ' (', acc_pct, '%)'))
hist(round(cleandfnew$pred_target, 0), xlab='Cases', main='Predictions (Zero-Inflated)')

# Hierarchical modeling

# Binomial on whether cases were sold or not
cleandf_binom <- cleandf %>%
    mutate(pos_cases=ifelse(TARGET > 0, T, F))
b_mod1 <- glm(pos_cases ~ . - TARGET, family=binomial(), data=cleandf_binom)
b_mod2 <- stepAIC(b_mod1, trace=F)
aic_b_mod1 <- AIC(b_mod1)
aic_b_mod2 <- AIC(b_mod2)

# Summary of binomial model 1:
summary(b_mod1)

# Summary of binomial model 2:
summary(b_mod2)

# AIC of binomial model 1:
aic_b_mod1

# AIC of binomial model 2:
aic_b_mod2

# Check overdispersion
cleandf_binom$pred_p <- ilogit(predict(b_mod2, cleandf_binom, interval='prediction'))
cleandf_binom$pred_pos_cases <- ifelse(cleandf_binom$pred_p > 0.5, T, F)
cleandf_binom$correct_pos_cases <- ifelse(cleandf_binom$pred_pos_cases == cleandf_binom$pos_cases, 1, 0)
acc_num <- sum(cleandf_binom$correct_pos_cases)
acc_pct <- round(100 * acc_num / nrow(cleandf_binom), 1)
print(paste0('Binomial model accuracy with p of 0.5: ', acc_num, ' of ', nrow(cleandf_binom), ' (', acc_pct, '%)'))

# ROC
roc_func <- function(data){
  temp_x <- rep(0, 101)
  temp_y <- rep(0, 101)
  temp_z <- rep(0, 101)
  temp_seq <- seq(from = 0, to = 1, by = 0.01)
  max_acc <- 0
  p_max_acc <- 0
  for (i in 1:length(temp_seq)){
    df <- data %>% 
        mutate(scored.class = as.logical(pred_p > temp_seq[i])) %>%
        mutate(true.class = as.logical(TARGET > 0)) %>%
        mutate(
           TP = ifelse(true.class == T & scored.class == T, 1, 0),
           FP = ifelse(true.class == F & scored.class == T, 1, 0),
           FN = ifelse(true.class == T & scored.class == F, 1, 0),
           TN = ifelse(true.class == F & scored.class == F, 1, 0)
        )
    TPR <- sum(df$TP)/(sum(df$TP) + sum(df$FN))
    FPR <- sum(df$FP)/(sum(df$FP) + sum(df$TN))
    acc <- (sum(df$TP) + sum(df$TN)) / nrow(df)
    if (acc > max_acc) {
        max_acc <- acc
        p_max_acc <- temp_seq[i]
    }
    temp_x[i] <- FPR
    temp_y[i] <- TPR
    temp_z[i] <- acc
  }
  temp_df <- bind_cols(temp_x, temp_y, temp_seq, temp_z) %>% as.data.frame()
  names(temp_df) <- c("FPR", "TPR", "p", "Accuracy")
  plt <- ggplot2::ggplot(data = temp_df, aes(x = FPR, y = TPR)) + geom_point() + geom_abline()
  plt2 <- ggplot2::ggplot(data = temp_df, aes(x = p, y = Accuracy)) + geom_point()
  AUC <- pracma::trapz(temp_x, temp_y)
  output <- list(plt, plt2, AUC, max_acc, p_max_acc)
  return(output)
}

# Generate ROC curve
rf <- roc_func(cleandf_binom)
print(rf)
rf[[5]]

# Find p at the maximum value for accuracy
cleandf_binom$pred_p <- ilogit(predict(b_mod2, cleandf_binom, interval='prediction'))
cleandf_binom$pred_pos_cases <- ifelse(cleandf_binom$pred_p > rf[[5]], T, F)
cleandf_binom$correct_pos_cases <- ifelse(cleandf_binom$pred_pos_cases == cleandf_binom$pos_cases, 1, 0)
acc_num <- sum(cleandf_binom$correct_pos_cases)
acc_pct <- round(100 * acc_num / nrow(cleandf_binom), 1)
print(paste0('Binomial model accuracy with p of ', rf[[5]], ': ', acc_num, ' of ', nrow(cleandf_binom), ' (', acc_pct, '%)'))

# Poisson against non-zero counts
p_mod3 <- glm(TARGET ~ ., family=poisson(), data=cleandf['TARGET' > 0])
aic_p_mod3 <- AIC(p_mod3)
p_mod4 <- stepAIC(p_mod3, trace=F)
aic_p_mod4 <- AIC(p_mod4)
print(paste0('AIC of poisson full model: ', aic_p_mod3))
print(paste0('AIC of poisson step-reduced model: ', aic_p_mod4))

# Check for overdispersion of the poisson models:
op3 <- p_mod3$deviance / p_mod3$df.residual
op4 <- p_mod4$deviance / p_mod4$df.residual
print(paste0('Poisson full model overdispersion: ', op3))
print(paste0('Poisson step-reduced model overdispersion: ', op4))

# Try negative binomial
nb_mod3 <- glm.nb(TARGET ~ ., data=cleandf['TARGET' > 0])
aic_nb_mod3 <- AIC(nb_mod3)
nb_mod4 <- stepAIC(nb_mod3, trace=F)
aic_nb_mod4 <- AIC(nb_mod4)
print(paste0('AIC of negative binomial full model: ', aic_nb_mod3))
print(paste0('AIC of negative binomial step-reduced model: ', aic_nb_mod4))

# Vary p to find the greatest accuracy
tmp_p <- seq(from = 0, to = 1, by = 0.01)
c_acc_pct <- c()
max_acc <- 0
p_max_acc <- 0
for (i in seq(1, length(tmp_p))) {

    # Make initial prediction on zero cases vs non-zero cases
    cleandf$pred_p <- ilogit(predict(b_mod2, cleandf, interval='prediction'))
    cleandf$pred_target <- ifelse(cleandf$pred_p > tmp_p[i], NA, 0)
    
    # Split df into zero cases and non-zero cases
    cleandf1 <- cleandf %>%
        filter(pred_target == 0)
    cleandf2 <- cleandf %>%
        filter(is.na(pred_target))
    
    # Make predications against non-zero cases
    cleandf2$pred_target <- round(exp(predict(p_mod4, cleandf2, interval ='prediction')), 0)
    cleandfnew <- rbind(cleandf1, cleandf2)
    
    # Calculate accuracy
    cleandfnew$correct <- ifelse(cleandfnew$pred_target == cleandfnew$TARGET, 1, 0)
    acc_num <- sum(cleandfnew$correct)
    acc_pct <- round(100 * acc_num / nrow(cleandfnew), 1)
    c_acc_pct <- c(c_acc_pct, acc_pct)
    if (acc_pct > max_acc) {
        max_acc <- acc_pct
        p_max_acc <- tmp_p[i]
    }
    #print(paste0('Hierarchical model accuracy: ', acc_num, ' of ', nrow(cleandfnew), ' (', acc_pct, '%)'))
}
plot(c_acc_pct ~ tmp_p)
print(paste0('Max accuracy is ', max_acc, ' when p=', p_max_acc))

# Now we'll use this cutoff value against the evaluation data.

# Predictions zero vs non-zero
cleandf_eval$pred_p <- ilogit(predict(b_mod2, cleandf_eval, interval='prediction'))
cleandf_eval$pred_target <- ifelse(cleandf_eval$pred_p > p_max_acc, NA, 0)

# Split df into zero cases and non-zero cases
cleandf_eval1 <- cleandf_eval %>%
    filter(pred_target == 0)
cleandf_eval2 <- cleandf_eval %>%
    filter(is.na(pred_target))

# Make predications against non-zero cases
cleandf_eval2$pred_target <- round(exp(predict(p_mod4, cleandf_eval2, interval ='prediction')), 0)
cleandf_eval_new <- rbind(cleandf_eval1, cleandf_eval2)

# Show results
cleandf_eval_new %>% head(10) %>% as_tibble()
hist(cleandf_eval_new$pred_target, xlab='Cases', main='Predicted Cases Sold')
write.csv(cleandf_eval, "wine_predictions2.csv", row.names = FALSE)
