head(dftrain)
summary(dftrain)
summary(dfeval)
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
gather(key = 'variable', value = 'value')
ggplot(plot_train) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
theme_ipsum() +
facet_wrap(. ~variable, scales='free', ncol=4)
# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
ncol(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(1, 15)) {
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
# For some reason R renamed the INDEX column to "ï..INDEX"
dftrain <- dftrain %>%
dplyr::select(-INDEX)
dfeval <- dfeval %>%
dplyr::select(-IN)
# Impute missing values in training data (except for STARS)
dftrain_imputed <- mice(dftrain %>% dplyr::select(-STARS), m=5, maxit=5, method='pmm')
cleandf <- complete(dftrain_imputed) %>%
mutate(STARS = dftrain$STARS)
# Impute missing values in eval data (except for STARS and TARGET)
dfeval_imputed <- mice(dfeval %>% dplyr::select(-STARS, -TARGET), m=5, maxit=5, method='pmm')
cleandf_eval <- complete(dfeval_imputed) %>%
mutate(STARS = dfeval$STARS, TARGET = dfeval$TARGET)
summary(cleandf)
summary(cleandf_eval)
p_mod1 <- glm(TARGET ~., family="poisson", data=cleandf)
summary(p_mod1)
p_mod2 <- stepAIC(p_mod1, trace = F)
summary(p_mod2)
lm_mod1 <- lm(TARGET ~., data = cleandf)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)
aic_p_mod1 <- p_mod1$aic
aic_p_mod1
aic_p_mod2 <- p_mod2$aic
aic_p_mod2
# Poisson - Minimum AIC
c(p_mod1$formula,p_mod2$formula)[which.min(c(p_mod1$aic,p_mod2$aic))]
r2_lm_mod1 <- summary(lm_mod1)$adj.r.squared
r2_lm_mod1
r2_lm_mod2 <- summary(lm_mod2)$adj.r.squared
r2_lm_mod2
# Multiple Linear Regression Model - Highest Adjusted R Squared
c(formula(lm_mod1),formula(lm_mod2))[which.max(c(summary(lm_mod1)$adj.r.squared, summary(lm_mod2)$adj.r.squared))]
mse <- function(df, model){
mean((df$TARGET - predict(model))^2)
}
mse_p_mod1 <- mse(cleandf, p_mod1)
mse_p_mod2 <- mse(cleandf, p_mod2)
mse_lm_mod1 <- get_mse(lm_mod1)
mse_lm_mod2 <- get_mse(lm_mod2)
models <- c("Possion Model 1", "Possion Model 2", "Linear Model 1", "Linear Model 2")
#rows <- c("Models", "MSE", "AIC")
MSE <- list(mse_p_mod1, mse_p_mod2, mse_lm_mod1, mse_lm_mod2)
AIC <- list(aic_p_mod1, aic_p_mod2, aic_lm_mod1, aic_lm_mod2)
knitr::kable(rbind(MSE, AIC), col.names = models)
prob2 <- predict(lm_mod2, cleandf_eval, interval ='prediction')
cleandf_eval$TARGET <- prob2[,1]
cleandf_eval %>% head(10) %>% as_tibble()
write.csv(cleandf_eval, "wine_predictions2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(tidyverse)
library(dplyr)
library(corrplot)
library(skimr)
library(DataExplorer)
library(ggplot2)
library(hrbrthemes)
library(mice)
library(MASS)
library(dvmisc)
library(gridExtra)
library(lattice)
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-training-data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-evaluation-data.csv")
head(dftrain)
summary(dftrain)
summary(dfeval)
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
gather(key = 'variable', value = 'value')
ggplot(plot_train) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
theme_ipsum() +
facet_wrap(. ~variable, scales='free', ncol=4)
# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
ncol(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(2, 16)) {
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
# For some reason R renamed the INDEX column to "ï..INDEX"
dftrain <- dftrain %>%
dplyr::select(-INDEX)
dfeval <- dfeval %>%
dplyr::select(-IN)
# Impute missing values in training data (except for STARS)
dftrain_imputed <- mice(dftrain %>% dplyr::select(-STARS), m=5, maxit=5, method='pmm')
cleandf <- complete(dftrain_imputed) %>%
mutate(STARS = dftrain$STARS)
# Impute missing values in eval data (except for STARS and TARGET)
dfeval_imputed <- mice(dfeval %>% dplyr::select(-STARS, -TARGET), m=5, maxit=5, method='pmm')
cleandf_eval <- complete(dfeval_imputed) %>%
mutate(STARS = dfeval$STARS, TARGET = dfeval$TARGET)
summary(cleandf)
summary(cleandf_eval)
p_mod1 <- glm(TARGET ~., family="poisson", data=cleandf)
summary(p_mod1)
p_mod2 <- stepAIC(p_mod1, trace = F)
summary(p_mod2)
lm_mod1 <- lm(TARGET ~., data = cleandf)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)
aic_p_mod1 <- p_mod1$aic
aic_p_mod1
aic_p_mod2 <- p_mod2$aic
aic_p_mod2
# Poisson - Minimum AIC
c(p_mod1$formula,p_mod2$formula)[which.min(c(p_mod1$aic,p_mod2$aic))]
r2_lm_mod1 <- summary(lm_mod1)$adj.r.squared
r2_lm_mod1
r2_lm_mod2 <- summary(lm_mod2)$adj.r.squared
r2_lm_mod2
# Multiple Linear Regression Model - Highest Adjusted R Squared
c(formula(lm_mod1),formula(lm_mod2))[which.max(c(summary(lm_mod1)$adj.r.squared, summary(lm_mod2)$adj.r.squared))]
mse <- function(df, model){
mean((df$TARGET - predict(model))^2)
}
mse_p_mod1 <- mse(cleandf, p_mod1)
mse_p_mod2 <- mse(cleandf, p_mod2)
mse_lm_mod1 <- get_mse(lm_mod1)
mse_lm_mod2 <- get_mse(lm_mod2)
models <- c("Possion Model 1", "Possion Model 2", "Linear Model 1", "Linear Model 2")
#rows <- c("Models", "MSE", "AIC")
MSE <- list(mse_p_mod1, mse_p_mod2, mse_lm_mod1, mse_lm_mod2)
AIC <- list(aic_p_mod1, aic_p_mod2, aic_lm_mod1, aic_lm_mod2)
knitr::kable(rbind(MSE, AIC), col.names = models)
prob2 <- predict(lm_mod2, cleandf_eval, interval ='prediction')
cleandf_eval$TARGET <- prob2[,1]
cleandf_eval %>% head(10) %>% as_tibble()
write.csv(cleandf_eval, "wine_predictions2.csv", row.names = FALSE)
ncol(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(2, 16)) {
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[1:6], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
ncol(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(2, 16)) {
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(tidyverse)
library(dplyr)
library(corrplot)
library(skimr)
library(DataExplorer)
library(ggplot2)
library(hrbrthemes)
library(mice)
library(MASS)
library(dvmisc)
library(gridExtra)
library(lattice)
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-training-data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-evaluation-data.csv")
head(dftrain)
summary(dftrain)
summary(dfeval)
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
gather(key = 'variable', value = 'value')
ggplot(plot_train) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
theme_ipsum() +
facet_wrap(. ~variable, scales='free', ncol=4)
# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
glimpse(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(2, 16)) {
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(tidyverse)
library(dplyr)
library(corrplot)
library(skimr)
library(DataExplorer)
library(ggplot2)
library(hrbrthemes)
library(mice)
library(MASS)
library(dvmisc)
library(gridExtra)
library(lattice)
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-training-data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-evaluation-data.csv")
head(dftrain)
summary(dftrain)
summary(dfeval)
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
gather(key = 'variable', value = 'value')
ggplot(plot_train) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
theme_ipsum() +
facet_wrap(. ~variable, scales='free', ncol=4)
# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
glimpse(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(3, 16)) {  # skip INDEX and TARGET variables
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[2:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
# For some reason R renamed the INDEX column to "ï..INDEX"
dftrain <- dftrain %>%
dplyr::select(-INDEX)
dfeval <- dfeval %>%
dplyr::select(-IN)
# Impute missing values in training data (except for STARS)
dftrain_imputed <- mice(dftrain %>% dplyr::select(-STARS), m=5, maxit=5, method='pmm')
cleandf <- complete(dftrain_imputed) %>%
mutate(STARS = dftrain$STARS)
# Impute missing values in eval data (except for STARS and TARGET)
dfeval_imputed <- mice(dfeval %>% dplyr::select(-STARS, -TARGET), m=5, maxit=5, method='pmm')
cleandf_eval <- complete(dfeval_imputed) %>%
mutate(STARS = dfeval$STARS, TARGET = dfeval$TARGET)
summary(cleandf)
summary(cleandf_eval)
p_mod1 <- glm(TARGET ~., family="poisson", data=cleandf)
summary(p_mod1)
p_mod2 <- stepAIC(p_mod1, trace = F)
summary(p_mod2)
lm_mod1 <- lm(TARGET ~., data = cleandf)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)
aic_p_mod1 <- p_mod1$aic
aic_p_mod1
aic_p_mod2 <- p_mod2$aic
aic_p_mod2
# Poisson - Minimum AIC
c(p_mod1$formula,p_mod2$formula)[which.min(c(p_mod1$aic,p_mod2$aic))]
r2_lm_mod1 <- summary(lm_mod1)$adj.r.squared
r2_lm_mod1
r2_lm_mod2 <- summary(lm_mod2)$adj.r.squared
r2_lm_mod2
# Multiple Linear Regression Model - Highest Adjusted R Squared
c(formula(lm_mod1),formula(lm_mod2))[which.max(c(summary(lm_mod1)$adj.r.squared, summary(lm_mod2)$adj.r.squared))]
mse <- function(df, model){
mean((df$TARGET - predict(model))^2)
}
mse_p_mod1 <- mse(cleandf, p_mod1)
mse_p_mod2 <- mse(cleandf, p_mod2)
mse_lm_mod1 <- get_mse(lm_mod1)
mse_lm_mod2 <- get_mse(lm_mod2)
models <- c("Possion Model 1", "Possion Model 2", "Linear Model 1", "Linear Model 2")
#rows <- c("Models", "MSE", "AIC")
MSE <- list(mse_p_mod1, mse_p_mod2, mse_lm_mod1, mse_lm_mod2)
AIC <- list(aic_p_mod1, aic_p_mod2, aic_lm_mod1, aic_lm_mod2)
knitr::kable(rbind(MSE, AIC), col.names = models)
prob2 <- predict(lm_mod2, cleandf_eval, interval ='prediction')
cleandf_eval$TARGET <- prob2[,1]
cleandf_eval %>% head(10) %>% as_tibble()
write.csv(cleandf_eval, "wine_predictions2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(tidyverse)
library(dplyr)
library(corrplot)
library(skimr)
library(DataExplorer)
library(ggplot2)
library(hrbrthemes)
library(mice)
library(MASS)
library(dvmisc)
library(gridExtra)
library(lattice)
dftrain <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-training-data.csv")
dfeval <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Homework_5/csv/wine-evaluation-data.csv")
head(dftrain)
summary(dftrain)
summary(dfeval)
str(dftrain)
str(dfeval)
plot_train <- dftrain %>%
gather(key = 'variable', value = 'value')
ggplot(plot_train) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
theme_ipsum() +
facet_wrap(. ~variable, scales='free', ncol=4)
# Create logical variable to indicate whether there is a star rating for this wine
dftrain <- dftrain %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
dfeval <- dfeval %>%
mutate(STARS=ifelse(is.na(STARS), 'NR', STARS))
glimpse(dftrain)
plt <- vector('list', ncol(dftrain) - 1)
for (i in seq(3, 16)) {  # skip INDEX and TARGET variables
if (class(dftrain[, i]) == 'numeric') {
tmpmin <- min(dftrain[, i], na.rm=T)
tmpinterval <- (max(dftrain[, i], na.rm=T) - tmpmin) / 5
tmpcuts <- c()
for (j in seq(1, 5)) {
tmpcuts <- c(tmpcuts, tmpmin + (j * tmpinterval))
}
#dftrain$x <- dftrain[, i] %>% cut(breaks=5, ordered_result=T, right=F)
dftrain$x <- dftrain[, i] %>% cut(breaks=tmpcuts, ordered_result=T, right=F)
} else {
dftrain$x <- dftrain[, i]
}
dftmp <- dftrain %>% group_by(x) %>% summarize(ct=sum(TARGET))
plt[[i]] <- barchart(dftmp$ct ~ dftmp$x, horiz=F, col='darkgreen', xlab=colnames(dftrain)[i], ylab='Cases')
}
dftrain <- subset(dftrain, select=-x)  # remove temporary variable
grid.arrange(grobs=plt[3:7], ncol=3, nrow=2)
grid.arrange(grobs=plt[8:13], ncol=3, nrow=2)
grid.arrange(grobs=plt[14:16], ncol=3, nrow=2)
# For some reason R renamed the INDEX column to "ï..INDEX"
dftrain <- dftrain %>%
dplyr::select(-INDEX)
dfeval <- dfeval %>%
dplyr::select(-IN)
# Impute missing values in training data (except for STARS)
dftrain_imputed <- mice(dftrain %>% dplyr::select(-STARS), m=5, maxit=5, method='pmm')
cleandf <- complete(dftrain_imputed) %>%
mutate(STARS = dftrain$STARS)
# Impute missing values in eval data (except for STARS and TARGET)
dfeval_imputed <- mice(dfeval %>% dplyr::select(-STARS, -TARGET), m=5, maxit=5, method='pmm')
cleandf_eval <- complete(dfeval_imputed) %>%
mutate(STARS = dfeval$STARS, TARGET = dfeval$TARGET)
summary(cleandf)
summary(cleandf_eval)
p_mod1 <- glm(TARGET ~., family="poisson", data=cleandf)
summary(p_mod1)
p_mod2 <- stepAIC(p_mod1, trace = F)
summary(p_mod2)
lm_mod1 <- lm(TARGET ~., data = cleandf)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)
aic_p_mod1 <- p_mod1$aic
aic_p_mod1
aic_p_mod2 <- p_mod2$aic
aic_p_mod2
# Poisson - Minimum AIC
c(p_mod1$formula,p_mod2$formula)[which.min(c(p_mod1$aic,p_mod2$aic))]
r2_lm_mod1 <- summary(lm_mod1)$adj.r.squared
r2_lm_mod1
r2_lm_mod2 <- summary(lm_mod2)$adj.r.squared
r2_lm_mod2
# Multiple Linear Regression Model - Highest Adjusted R Squared
c(formula(lm_mod1),formula(lm_mod2))[which.max(c(summary(lm_mod1)$adj.r.squared, summary(lm_mod2)$adj.r.squared))]
mse <- function(df, model){
mean((df$TARGET - predict(model))^2)
}
mse_p_mod1 <- mse(cleandf, p_mod1)
mse_p_mod2 <- mse(cleandf, p_mod2)
mse_lm_mod1 <- get_mse(lm_mod1)
mse_lm_mod2 <- get_mse(lm_mod2)
models <- c("Possion Model 1", "Possion Model 2", "Linear Model 1", "Linear Model 2")
#rows <- c("Models", "MSE", "AIC")
MSE <- list(mse_p_mod1, mse_p_mod2, mse_lm_mod1, mse_lm_mod2)
AIC <- list(aic_p_mod1, aic_p_mod2, aic_lm_mod1, aic_lm_mod2)
knitr::kable(rbind(MSE, AIC), col.names = models)
prob2 <- predict(lm_mod2, cleandf_eval, interval ='prediction')
cleandf_eval$TARGET <- prob2[,1]
cleandf_eval %>% head(10) %>% as_tibble()
write.csv(cleandf_eval, "wine_predictions2.csv", row.names = FALSE)
