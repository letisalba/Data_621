---
title: "Data 621 - Blog 3"
author: "Leticia Salazar"
date: "November 14, 2022"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    theme: paper
    highlight: kate
    toc: yes
    toc_float: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$~$

## Multiple Linear Regression

The second blog demonstrated how to create a simple linear regression model to find relationships between two quantitative variables where there's one dependent variable (y) and one independent variable (x). In this third blog I am introducing multiple linear regression, where there's still one dependent variable but one or more independent variables. 

Below I'll demonstrate how to start a multiple linear regression model using the `diamonds` package from the library `ggplot2`. For a more in depth multiple regression model I'd like to share the work my team and I create for a [homework](https://rpubs.com/letisalba/data621_hwk1). 

### Load Libraries

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(jtools)
library(hrbrthemes)
library(gtsummary)
```


### Loading Data

Loading data using the `head()` function
```{r, echo= FALSE}
head(diamonds)
```

$~$

Using `summary()` function to get the statistical structure of our data
```{r, echo=FALSE}
summary(diamonds)
```

$~$

Using `gtsummary()` function as well to view the data's structure differently
```{r, echo=FALSE}
tbl_summary(diamonds)
```

$~$

The `diamonds` data set does not require any cleaning or transformation in order to use but always keep in mind the data sets we use in the future will.

$~$

### Model Building 

Just like in simple linear regression, we start by creating a model to look at the overall data. This serves as a base model to compare our other models to. The equation for a multiple linear regression model is: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 ... + \beta_p x_p + \epsilon$

```{r}
# creating first model
model <- lm(diamonds)
summary(model)
```

Plotting our models is the easiest part, first you'll want to create a matrix of nrows x ncols so that the plots are fitted into one page. Using the `plot()` function we can insert our model name and we get 4 different plots:

```{r}
# plots
par(mfrow=c(2,2)) # matrix of plots
plot(model) # create plots for model
```
$~$

1. **Residuals vs Fitted**: scatter plot of the residuals on the y-axis and fitted values on the x-axis. The plot is used to detect non-linearity, whether homoskedasticity holds and outliers.
2. **Normal Q-Q**:  scatter plot of two sets of quantiles against one another. The theoretical quantities helps us assess if a set of data came from the theoretical distribution such as a Normal or exponential.
3. **Scale-Location**: scatter plot showing the spread of the residuals along the ranges of the predictors. Similarly to the residuals vs fitted it simplifies the analysis of the homoskedasticity assumption.
4. **Residuals vs. Leverage**: scatter plot that looks at the spread of the standardized residuals and it's changes in leverage or sensitivity of the fitted. It can also be used to detect heteroskedasticity and non-linearity. It takes into consideration Cook's distance to detect any points that have influence on the model.

With multiple linear regression there are assumptions to keep in mind:

* Linearity of the relationships between the dependent and independent variables
* Independence of the observations
* Normality of the residuals
* Homoscedasticity of the residuals
* No influential points (outliers)
* No multicollinearity  arises when there is a strong linear correlation between the independent variables, conditional on the other variables in the model. It is important to check it because it may lead to an imprecision or an instability of the estimated parameters when a variable changes.


$~$

The second model we create, similar to the first, will be with less variables to check if this model is fits better.

```{r, echo=FALSE}
model2 <- lm(price ~ carat + cut + depth + table, diamonds)
summary(model2)
```


**Plots for model2**

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(model2)
```
$~$

The next two models (`model3` and `model4`) are created using backwards elimination to view the differences in statistical output we receive and compare the adjusted $R^2$. 

**model3**

```{r, echo=FALSE}
model3 <- lm(price ~ carat + color + clarity + x + y, diamonds)
summary(model3)
```

**Plots for model3**

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(model3)
```

**model4**

```{r, echo=FALSE}
model4 <- lm(price ~ carat + clarity + x, diamonds)
summary(model4)
```

**Plots of model4**

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(model4)
```

$~$

After plotting our models and analyzing the summary outputs we conclude that the model that better fits our data is `model3`. Comparing statistical outputs, `model3` had the highest $R^2$ at .9171, compared to `model2` at .8571 and `model4` at .897. While this simple analysis works with our data, a deeper analysis will always help you select the best fit model for your data, perform predictions and more.

If you'd like to dig deeper into multiple linear regression I found a great article from  [R-Bloggers](https://www.r-bloggers.com/2021/10/multiple-linear-regression-made-simple/) that takes you step by step on the process to create your own models.

$~$

#### References:
* Assumptions of multiple linear regression | by MD Sohel Mahmood ... (n.d.). Retrieved November 14, 2022, from https://towardsdatascience.com/assumptions-of-multiple-linear-regression-d16f2eb8a2e7 
* Alex. (2022, February 2). Linear regression plots: Fitted vs residuals. Boostedml. Retrieved November 14, 2022, from https://boostedml.com/2019/03/linear-regression-plots-fitted-vs-residuals.html 
* Alex. (2020, May 20). The QQ plot in linear regression. Boostedml. Retrieved November 14, 2022, from https://boostedml.com/2019/03/linear-regression-plots-how-to-read-a-qq-plot.html 
* Alex. (2020, May 20). The scale location plot: Interpretation in R. Boostedml. Retrieved November 14, 2022, from https://boostedml.com/2019/03/linear-regression-plots-scale-location-plot.html 
* Alex. (2020, September 10). Linear regression plots: Residuals vs leverage. Boostedml. Retrieved November 14, 2022, from https://boostedml.com/2019/03/linear-regression-plots-residuals-vs-leverage.html 
* Multiple linear regression made simple. Stats and R. (n.d.). Retrieved November 14, 2022, from https://statsandr.com/blog/multiple-linear-regression-made-simple/#multiple-linear-regression 

