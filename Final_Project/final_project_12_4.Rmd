---
output:
  html_document:
    theme: flatly
    highlight: textmate
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **House Prices**


### CUNY SPS DATA 621

#### GROUP 2: William Aiken, Donald Butler, Michael Ippolito, Bharani Nittala, and Leticia Salazar

------------------------------------------------------

START OF OUR CODING HERE! WILL NOT SHOW FOR PDF VERSION

### Load Libraries:

These are the libraries used to explore, prepare, analyze and build our models
```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(dplyr)
library(corrplot)
library(MASS)
library(dvmisc)
library(car)
library(lmtest)
library(olsrr)
library(caret)
```

### Load Data:

We have included the original data sets in our GitHub account and read from this location. Since our data set doesn't come with a training and evaluation data sets we will be splitting our data using the 70% - 30% split. Below we are showing the training data set:
```{r, echo=FALSE}
house_prices <- read.csv("https://raw.githubusercontent.com/letisalba/Data_621/master/Final_Project/csv/Housing.csv")

# make this example reproducible
set.seed(1)

# use 70% of data set as training set and 30% as evaluation set
sample <- sample(c(TRUE, FALSE), nrow(house_prices), replace=TRUE, prob=c(0.7,0.3))
dftrain  <- house_prices[sample, ]
dfeval   <- house_prices[!sample, ]

head(dftrain)
```

$~$

### Data Exploration:

Based on this our training data includes 386 records and 13 variables whereas the evaluation data includes 159 records and 13 variables.

Training:
```{r, echo=FALSE}
str(dftrain)
```

$~$

Evaluation:
```{r, echo=FALSE}
str(dfeval)
```

$~$

Using the `summary()` function lets start exploring the training and evaluation data.

Training:
```{r, echo=FALSE}
summary(dftrain)
```

$~$

Evaluation:
```{r, echo=FALSE}
summary(dfeval)
```

$~$

#### Price:  
It is important to recognize that this dataset contains homes with prices above 1 million. It is not clear that this is a US dataset, which would indicate that this is for luxury homes and/or high value markets.

```{r price, echo=FALSE, warning=FALSE, error=FALSE}
options(scipen=5)
hist(dftrain$price / 10^6, main = 'Distribution of Price', xlab = 'Price in millions')
```
$~$

#### Area:  
The area variable appears to be square footage of the home. We would traditionally expect that increases in area would lead to increases in price.

```{r area, echo=FALSE, warning=FALSE, error=FALSE}
hist(dftrain$area, main = 'Distribution of Area', xlab = 'Area in Square Feet')

```

$~$

#### Bedrooms:  
While we expect increases in the number of bedrooms to increase the price, we also realize that at some point there are diminishing returns that an additional bedroom doesn't have as much of an impact. For example, increasing from one to two bedrooms should have significant increase in price, while increasing from four to five, perhaps not so much.

```{r bedrooms, echo=FALSE}
dftrain %>% count(bedrooms)
```

Based on the distribution of the number of Bedrooms, it may be best to categorize these with dummy variables; 2, 3, and 4+.

$~$

#### Bathrooms:  
Similar to the number of bedrooms, we would expect that an increase in bathroom count would lead to increases in price. Although similarly, having more than four bathrooms is likely going to lead to smaller increases.

```{r bathrooms, echo=FALSE}
dftrain %>% count(bathrooms)
```

Based on the distribution of the number of bathrooms, it may be best to categorize these with dummy variables; 2, and 3+.

$~$

#### Stories:  
Similar to the number of bedrooms and bathrooms, it would seem to make sense to classify homes with 3 or more floors together by introducing dummy variables; 2, and 3+.

```{r stories, echo=FALSE}
dftrain %>% count(stories)
```

$~$

#### Parking:  
We are assuming that the parking variable represents the size of a garage. Similar to other variable the increase in price from no garage to a one car garage would be significant, while additional cars would add some lesser value. It would initially seem to make sense to introduce dummy variables; 1, and 2+.

```{r parking, echo=FALSE}
dftrain %>% count(parking)
```

$~$

#### Furnishing Status:  
The furnishing status variable is taking on three values; unfurnished, semi-furnished, and furnished. Since we would consider unfurnished as the default state, we will use dummy variables; semi-furnished and furnished.

```{r furnishingstatus, echo=FALSE}
dftrain %>% count(furnishingstatus)
```

$~$

#### Main Road:  
The main road variable is yes/no based on the street of the home. We will replace this with a dummy variable.

```{r mainroad, echo=FALSE}
dftrain %>% count(mainroad)
```

$~$

#### Guest Room:  
The guest room variable is yes/no based on the home having a guest room. It is unclear from the dataset source if this is in addition to the number of bedrooms, but we would expect houses with a guest room to have a higher price. We will replace this with a dummy variable.

```{r guestroom, echo=FALSE}
dftrain %>% count(guestroom)
```

$~$

#### Basement:  
The basement variable is yes/no based on the home having a basement. It is unclear if having a basement or not would lead to an increase in home price, but we will replace this with a dummy variable for analysis.

```{r basement, echo=FALSE}
dftrain %>% count(basement)
```

$~$

#### Hot Water Heating:  
Based on the distribution, we assume that the hot water heating variable represents if the house has in-floor heating, rather than forced air. Based on this assumption, we assume that having this feature would lead to higher house price. The variable will be replaced with a dummy variable for analysis.

```{r hotwaterheating, echo=FALSE}
dftrain %>% count(hotwaterheating)
```

$~$

#### Air Conditioning:  
The air conditioning variable indicates if the house has central air conditioning. We would expect homes with air conditioning would have a higher price than those without. The variable will be replaced with a dummy variable.

```{r airconditioning, echo=FALSE}
dftrain %>% count(airconditioning)
```

$~$

#### Preferential Area:  
The dataset source doesn't specify exactly what this variable represents. We are assuming that this is a yes/no value if the house is in a preferred neighborhood. We would expect houses with a yes to be higher price than those not.

```{r prefarea, echo=FALSE}
dftrain %>% count(prefarea)
```

$~$

### Data Preparation:

Based on our exploration, we do not have any blank values in our dataset. 

#### Clean Function:  
We will introduce a clean function to replace our categorical variables with the dummy values. This will also ensure that our test and train datasets are processed in the same way.

```{r clean, echo=FALSE, warning=FALSE, message=FALSE}
fun_clean_df <- function(df) {
  df <- df %>%
    mutate(bed2 = ifelse(bedrooms == 2,1,0)) %>%
    mutate(bed3 = ifelse(bedrooms == 3,1,0)) %>%
    mutate(bed4 = ifelse(bedrooms == 4,1,0)) %>%
    mutate(bed5 = ifelse(bedrooms == 5,1,0)) %>%
    mutate(bed6plus = ifelse(bedrooms >= 6,1,0)) %>% dplyr::select(-bedrooms) %>%
    mutate(bath2 = ifelse(bathrooms == 2,1,0)) %>%
    mutate(bath3 = ifelse(bathrooms == 3,1,0)) %>%
    mutate(bath4plus = ifelse(bathrooms >= 4,1,0)) %>% dplyr::select(-bathrooms) %>%
    mutate(floor2 = ifelse(stories == 2,1,0)) %>%
    mutate(floor3 = ifelse(stories == 3,1,0)) %>%
    mutate(floor4plus = ifelse(stories >= 4,1,0)) %>% dplyr::select(-stories) %>%
    mutate(car1 = ifelse(parking == 1,1,0)) %>%
    mutate(car2 = ifelse(parking == 2,1,0)) %>% 
    mutate(car3plus = ifelse(parking >= 3,1,0)) %>% dplyr::select(-parking) %>%
    mutate(semifurnished = ifelse(furnishingstatus == 'semi-furnished',1,0)) %>%
    mutate(furnished = ifelse(furnishingstatus == 'furnished',1,0)) %>% dplyr::select(-furnishingstatus) %>%
    mutate(mainroad = ifelse(mainroad == 'yes',1,0)) %>%
    mutate(guestroom = ifelse(guestroom == 'yes',1,0)) %>%
    mutate(basement = ifelse(basement == 'yes',1,0)) %>%
    mutate(hotwaterheating = ifelse(hotwaterheating == 'yes',1,0)) %>%
    mutate(ac = ifelse(airconditioning == 'yes',1,0)) %>% dplyr::select(-airconditioning) %>%
    mutate(neighborhood = ifelse(prefarea == 'yes',1,0)) %>% dplyr::select(-prefarea)
}

dftrain_clean <- fun_clean_df(dftrain)
dfeval_clean <- fun_clean_df(dfeval)

```

$~$

### Data Visualization:

Visual evaluation:
```{r, echo=FALSE, fig.width=12}

# Compare quantitative variables
pairs(dftrain[, c('price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking')])
corrplot(cor(dftrain[, c('price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking')], use = 'complete.obs'), tl.cex = 0.5)

# Compare nominal categorical variables to price
par(mfrow=c(3, 3))
boxplot(price ~ mainroad, data=dftrain)
boxplot(price ~ guestroom, data=dftrain)
boxplot(price ~ basement, data=dftrain)
boxplot(price ~ hotwaterheating, data=dftrain)
boxplot(price ~ airconditioning, data=dftrain)
boxplot(price ~ prefarea, data=dftrain)
boxplot(price ~ furnishingstatus, data=dftrain)

# Compare ordinal categorical variables
par(mfrow=c(2, 2))
boxplot(price ~ bedrooms, data=dftrain)
boxplot(price ~ bathrooms, data=dftrain)
boxplot(price ~ stories, data=dftrain)
boxplot(price ~ parking, data=dftrain)

```

$~$

#### Histogram of house prices

```{r histogram, warning=FALSE, message=FALSE, echo=FALSE}

# Draw a histogram to figure out the distribution of Sale Price
options(scipen=10000)
ggplot(dftrain_clean, aes(x = price, fill = ..count..)) +
  geom_histogram() +
  ggtitle("Figure 1 Histogram of Price") +
  ylab("Count of houses") +
  xlab("Housing Price") + 
  theme(plot.title = element_text(hjust = 0.9))

```

$~$

#### Correlation Plot:  

After cleaning the dataset looking at a correlation plot will give us confirmation about our initial examination for the variables.

```{r corplot, warning=FALSE, message=FALSE}
cor_res <- cor(dftrain_clean)
corrplot(cor_res,
         type = "lower",
         order = "original",
         tl.col = "black",
         tl.srt = 50,
         tl.cex = 1)
```

The correlation plot generally confirms our initial expectations for the data. 


$~$

### Model Building:

#### Multiple Linear Regression Models

```{r, echo=FALSE}
#partition data
#set.seed(10000)
#train.index <- sample(c(1:dim(dftrain_clean)[1]), dim(dftrain_clean)[1]*0.8)
#model_lin_train = dftrain_clean[train.index,]
#model_lin_valid <- dftrain_clean[-train.index,]
model_lin_train <- dftrain_clean
```

$~$

#### MLR Model 1
```{r, echo=FALSE}
lm_mod1 <- lm(price ~., data = model_lin_train)
aic_lm_mod1 = AIC(lm_mod1)
summary(lm_mod1)
```

$~$

#### MLR Model 2
```{r, echo=FALSE}
lm_mod2 <- stepAIC(lm_mod1, trace = F)
aic_lm_mod2 = AIC(lm_mod2)
summary(lm_mod2)
```

$~$

#### MLR Model 3
```{r, echo=FALSE, warning=FALSE}
# reduce collinearity, and remove low values

lm_mod3 <- lm(price ~ area + guestroom + basement + bath2 + bath3 + 
    bath4plus + floor2 + floor3 + floor4plus + car1 + car2 + 
    car3plus + semifurnished + furnished + ac + neighborhood 
    - car3plus - bed2, 
    data = model_lin_train)
summary(lm_mod3)
```

$~$

### Model Selection:

Verify linear modeling assumptions:
```{r, fig.width=12, echo=FALSE, warning=FALSE, message=FALSE}

# Define function to calculate mean squared error
calc_mse <- function(lmod) {
  return(mean((summary(lmod))$residuals ^ 2))
}

# Define function to aid in model analysis
ModelAnalysis <- function(lmod) {

  # Plot residuals
  print('--------------------------------------------------')
  print(lmod$call)
  par(mfrow=c(2,2))
  plot(lmod)
  print('')

  # Shapiro test to determine normality of residuals
  # Null hypothesis: the residuals are normal.
  # If the p-value is small, reject the null, i.e., consider the residuals *not* normally distributed.
  if (length(lmod$fitted.values) > 3 & length(lmod$fitted.values) < 5000) {
      st <- shapiro.test(lmod$residuals)
      if (st$p.value <= 0.05) {
        print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is <= 0.05, so reject the null; i.e., the residuals are NOT NORMAL"))
      } else {
        print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is > 0.05, so do not reject the null; i.e., the residuals are NORMAL"))
      }
      print('')
  } else {
      print("Shapiro test for normality of residuals cannot be performed; sample length must be between 3 and 5000.")
  }
     
  # Breusch-Pagan test to determine homoschedasticity of residuals
  # Null hypothesis: the residuals are homoschedastic.
  # If the p-value is small, reject the null, i.e., consider the residuals heteroschedastic.
  bp <- bptest(lmod)
  if (bp$p.value > 0.05 & bp$statistic < 10) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is > 0.05 and the test statistic of ", bp$statistic,
          " is < 10, so don't reject the null; i.e., the residuals are HOMOSCHEDASTIC."))
  } else if (bp$p.value <= 0.05) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is <= 0.05 and the test statistic is ", bp$statistic,
          ", so reject the null; i.e., the residuals are HETEROSCHEDASTIC."))
  } else {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " and test statistic of ", bp$statistic,
          " are inconclusive, so homoschedasticity can't be determined using this test. But since the p-value is > 0.05, ",
          "it is reasonable to conclude that the residuals are HOMOSCHEDASTIC."))
  }
  print('')

  # Visually look for colinearity - dont do this for large models
  #pairs(model.matrix(lmod))

  # Variance inflation factor (VIF)
  print('Variance inflation factor (VIF)')
  print('<=1: not correlated, 1-5: moderately correlated, >5: strongly correlated')
  print(sort(vif(lmod), decreasing=T))
  print('')
  
  # Standardized residual plots (look for points outside of 2 or 3 stdev)
  p <- length(summary(lmod)$coeff[,1] - 1)  # number of model parameters
  stanres <- rstandard(lmod)
  for (i in seq(1, ceiling(p / 4))) {
    par(mfrow=c(2,2))
    starti <- ((i - 1) * 4) + 1
    for (j in seq(starti, starti + 3)) {
      if (j + 1 <= ncol(model.matrix(lmod))) {
        # Skip these plots since we're pretty sure that a linear model isn't valid here
        #plot(model.matrix(lmod)[, j + 1], stanres, xlab=colnames(model.matrix(lmod))[j + 1], ylab='Standardized residuals')
        #abline(h=c(-2, 2), lt=3, col='blue')
        #abline(h=c(-3, 3), lt=2, col='red')
      }
    }
  }
  
  # Model scores
  print('Model scores:')
  print(paste0('    adjusted R-squared: ', round(summary(lmod)$adj.r.squared, 3)))
  print(paste0('    AIC: ', round(AIC(lmod, k=2), 3)))
  print(paste0('    BIC: ', round(BIC(lmod), 3)))
  print(paste0('    Mallow\'s Cp: ', round(ols_mallows_cp(lmod, fullmodel=lmod), 3)))
  print(paste0('    mean squared error: ', round(calc_mse(lmod), 3)))
  print('')
  
  # Find leverage point cutoff
  n <- length(lmod$residuals)
  cutoff <- 2 * (p + 1) / n
  print(paste0('Leverage point cutoff: ', cutoff))
  print('')

  # Show points of influence
  print('First 10 points of influence:')
  poi <- lm.influence(lmod)$hat
  len_poi <- length(poi)
  ct <- 0
  for (i in seq(1, length(poi))) {
    if (poi[i] > cutoff) {
      ct <- ct + 1
      print(paste0('    case #', i, ': ', round(poi[i], 3)))
    }
        if (ct > 10) {
            break
        }
  }
  print('')
  
}

# Analysis on the two step-reduced models
ModelAnalysis(lm_mod2)
ModelAnalysis(lm_mod3)

```

$~$

Due to non-normal distribution and heteroschedasticity of residuals, try a transform. Use Box-Cox to estimate what kind of transform is appropriate.
```{r, echo=FALSE}

# Box-Cox transform on price
bc1 <- powerTransform(price ~ ., data=model_lin_train)
bc1

# Box-Cox result suggests doing a log transform on price
lm_mod4 <- lm(log(price) ~ ., data=model_lin_train)
summary(lm_mod4)
lm_mod5 <- stepAIC(lm_mod4, trace=F)
summary(lm_mod5)
ModelAnalysis(lm_mod5)

```

$~$

Investigate outliers.
```{r, echo=FALSE}

# Investigate top outliers
model_lin_train[c(2, 5, 9, 25, 49, 62, 77, 103, 110, 136, 210),]

```

$~$

Investigation of outliers reveals no obvious pattern, so we have to assume there is some other variable at play that we don't have data for (e.g. high-end appliances, presence of a pool, property condition, etc). Well remove the outliers and re-run model.
```{r, echo=FALSE}

# Log transform on price with outliers removed
lm_mod6 <- lm(formula(lm_mod5), 
    data = model_lin_train[c(-2, -5, -9, -25, -49, -62, -77, -103, -110, -136, -210),])
summary(lm_mod6)
lm_mod7 <- stepAIC(lm_mod6, trace=F)
summary(lm_mod7)
ModelAnalysis(lm_mod7)


```

$~$

Residuals are still not normally distributed. Use robust regression to try addressing non-normality.
```{r, echo=FALSE}

# Huber robust linear regression
lm_mod8 <- rlm(formula(lm_mod7), data=model_lin_train)
dftmp <- data.frame(cbind(price=model_lin_train$price, huber_weight=lm_mod8$w))
dftmp <- dftmp %>% arrange(huber_weight, ascending=F)
hist(dftmp$huber_weight, xlab='Huber weight', main='Histogram of Huber Weights')

# New linear model using Huber weights
lm_mod9 <- lm(formula(lm_mod7), weights=lm_mod8$w, data=model_lin_train)
summary(lm_mod9)
ModelAnalysis(lm_mod9)

# Remove most significant outliers
lm_mod10 <- lm(formula(lm_mod7), weights=lm_mod8$w[c(-53, -68, -87, -90, -103)], 
    data=model_lin_train[c(-53, -68, -87, -90, -103),])
summary(lm_mod10)
ModelAnalysis(lm_mod10)

```

$~$

Perform five-fold cross validation to validate our results.
```{r, echo=FALSE}

# Five-fold cross validation
set.seed(777)
tc <- trainControl(method = "cv", number = 5)
lmcv <- train(formula(lm_mod8), weights=lm_mod8$w, data=model_lin_train, method="lm", trControl=tc)
print(lmcv)
summary(lmcv)

```

$~$

Run selected model against validation set.
```{r, echo=FALSE}

# Huber robust linear regression
lm_valid1 <- lm(formula(lm_mod7), data=dfeval_clean)

# New linear model using Huber weights
lm_valid2 <- lm(formula(lm_mod7), weights=lm_valid1$w, data=dfeval_clean)
summary(lm_valid2)
ModelAnalysis(lm_valid2)

```

$~$

Our model comparison below:
```{r, echo=FALSE}

# Model comparison
hdr <- c('#', 'Train/Validation', 'Linear/Robust', 'Full/Step-reduced', 'Log Transform', 
    'Outliers Removed', 'Huber-Weighted', 'Adj R-Sqr')
f1 <- c(seq(1:11))
f2 <- c(rep('Train', 10), 'Validation')
f3 <- c(rep('Linear', 7), 'Robust', rep('Linear', 3))
f4 <- c('Full', 'Step', 'Step', 'Full', rep('Step', 7))
f5 <- c(rep('', 3), rep('Yes', 8))
f6 <- c(rep('', 5), 'Yes', 'Yes', '', '', 'Yes', '')
f7 <- c(rep('', 8), rep('Yes', 3))
f8 <- round(c(
    summary(lm_mod1)$adj.r.squared, 
    summary(lm_mod2)$adj.r.squared, 
    summary(lm_mod3)$adj.r.squared, 
    summary(lm_mod4)$adj.r.squared, 
    summary(lm_mod5)$adj.r.squared, 
    summary(lm_mod6)$adj.r.squared, 
    summary(lm_mod7)$adj.r.squared, 
    NA, 
    summary(lm_mod9)$adj.r.squared, 
    summary(lm_mod10)$adj.r.squared, 
    summary(lm_valid2)$adj.r.squared), 3)
dfresult <- data.frame(f1, f2, f3, f4, f5, f6, f7, f8)
colnames(dfresult) <- hdr
dfresult


```

